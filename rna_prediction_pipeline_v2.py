#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¯çŠ¶RNAç»„ç»‡è¡¨è¾¾é¢„æµ‹ - æ”¹è¿›ç‰ˆPipeline v2.0
åŸºäºåŸå§‹é«˜åˆ†æ¨¡å‹(0.36)çš„æ”¹è¿›ç‰ˆæœ¬

ä¸»è¦æ”¹è¿›:
1. æ›´å¤šç”Ÿç‰©å­¦ç‰¹å¾å·¥ç¨‹ (åºåˆ—å¤æ‚åº¦ã€k-meré¢‘ç‡ã€äºŒçº§ç»“æ„é¢„æµ‹)
2. é«˜çº§ç‰¹å¾é€‰æ‹© (é€’å½’ç‰¹å¾æ¶ˆé™¤ã€LASSOã€äº’ä¿¡æ¯)
3. æ•°æ®å¢å¼ºå’Œä¸å¹³è¡¡å¤„ç†
4. æ›´ç²¾ç»†çš„è¶…å‚æ•°ç½‘æ ¼
5. å¤šå±‚Stackingå’ŒBlendingé›†æˆ
6. æ¨¡å‹æ ¡å‡†å’Œé˜ˆå€¼ä¼˜åŒ–
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# åŸºç¡€åº“
import time
import pickle
from datetime import datetime
from collections import Counter
import itertools
from scipy import stats
from scipy.stats import entropy

# æœºå™¨å­¦ä¹ åº“
from sklearn.model_selection import (
    StratifiedKFold, cross_val_score, RandomizedSearchCV, GridSearchCV
)
from sklearn.preprocessing import (
    LabelEncoder, StandardScaler, RobustScaler, MinMaxScaler,
    PolynomialFeatures, PowerTransformer
)
from sklearn.feature_selection import (
    SelectKBest, f_classif, mutual_info_classif,
    RFE, SelectFromModel, VarianceThreshold
)
from sklearn.ensemble import (
    RandomForestClassifier, ExtraTreesClassifier,
    VotingClassifier, StackingClassifier, BaggingClassifier
)
from sklearn.linear_model import (
    LogisticRegression, Lasso, ElasticNet
)
from sklearn.metrics import (
    f1_score, classification_report, confusion_matrix,
    make_scorer
)
from sklearn.calibration import CalibratedClassifierCV
from sklearn.utils.class_weight import compute_class_weight
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.neighbors import NearestNeighbors

# é«˜çº§æ¨¡å‹
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("è­¦å‘Š: XGBoostæœªå®‰è£…")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("è­¦å‘Š: LightGBMæœªå®‰è£…")

try:
    from catboost import CatBoostClassifier
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    print("è­¦å‘Š: CatBoostæœªå®‰è£…")

class AdvancedRNAPredictionPipeline:
    """
    æ”¹è¿›ç‰ˆç¯çŠ¶RNAç»„ç»‡è¡¨è¾¾é¢„æµ‹Pipeline
    """
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        np.random.seed(random_state)
        
        # æ•°æ®å­˜å‚¨
        self.train_data = None
        self.test_data = None
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.test_ids = None
        
        # é¢„å¤„ç†å™¨
        self.encoders = {}
        self.scalers = {}
        self.feature_selector = None
        self.target_encoder = LabelEncoder()
        
        # æ¨¡å‹ç›¸å…³
        self.models = {}
        self.optimized_models = {}
        self.model_results = []
        self.final_model = None
        
        # ç‰¹å¾åç§°
        self.feature_names = []
        self.original_features = []
        self.engineered_features = []
        
    def load_data(self, train_path='train.csv', test_path='test.csv'):
        """
        åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        """
        print("åŠ è½½æ•°æ®...")
        print("=" * 50)
        
        # åŠ è½½æ•°æ®
        self.train_data = pd.read_csv(train_path)
        self.test_data = pd.read_csv(test_path)
        
        print(f"è®­ç»ƒé›†å½¢çŠ¶: {self.train_data.shape}")
        print(f"æµ‹è¯•é›†å½¢çŠ¶: {self.test_data.shape}")
        
        # ä¿å­˜æµ‹è¯•é›†ID
        self.test_ids = self.test_data['ID'].values
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        print("\næ•°æ®è´¨é‡æ£€æŸ¥:")
        print(f"è®­ç»ƒé›†ç¼ºå¤±å€¼: {self.train_data.isnull().sum().sum()}")
        print(f"æµ‹è¯•é›†ç¼ºå¤±å€¼: {self.test_data.isnull().sum().sum()}")
        
        return self
    
    def analyze_data(self):
        """
        æ•°æ®åˆ†æå’Œæ¢ç´¢
        """
        print("\næ•°æ®åˆ†æ...")
        print("=" * 50)
        
        # ç›®æ ‡å˜é‡åˆ†å¸ƒ
        target_dist = self.train_data['Tissue'].value_counts()
        print("\nç›®æ ‡å˜é‡åˆ†å¸ƒ:")
        for tissue, count in target_dist.items():
            print(f"  {tissue}: {count} ({count/len(self.train_data)*100:.1f}%)")
        
        # ç‰¹å¾ç»Ÿè®¡
        numeric_features = self.train_data.select_dtypes(include=[np.number]).columns.tolist()
        if 'ID' in numeric_features:
            numeric_features.remove('ID')
        
        print(f"\næ•°å€¼ç‰¹å¾æ•°é‡: {len(numeric_features)}")
        print(f"åˆ†ç±»ç‰¹å¾æ•°é‡: {len(self.train_data.columns) - len(numeric_features) - 2}")  # å‡å»IDå’ŒTissue
        
        # ä¿å­˜åŸå§‹ç‰¹å¾å
        self.original_features = [col for col in self.train_data.columns if col not in ['ID', 'Tissue']]
        
        return self
    
    def advanced_feature_engineering(self):
        """
        é«˜çº§ç‰¹å¾å·¥ç¨‹
        """
        print("\né«˜çº§ç‰¹å¾å·¥ç¨‹...")
        print("=" * 50)
        
        # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹
        train_features = self.train_data.drop(['Tissue'], axis=1)
        all_data = pd.concat([train_features, self.test_data], ignore_index=True)
        
        print(f"åŸå§‹ç‰¹å¾æ•°é‡: {len(self.original_features)}")
        
        # 1. åŸºç¡€ç‰¹å¾å·¥ç¨‹ (ä¿ç•™åŸæœ‰æˆåŠŸç‰¹å¾)
        print("1. åŸºç¡€ç‰¹å¾å·¥ç¨‹...")
        
        # è®¡ç®—ATå«é‡
        if 'A_ratio' in all_data.columns and 'T_ratio' in all_data.columns:
            all_data['AT_content'] = all_data['A_ratio'] + all_data['T_ratio']
        
        # GC/ATæ¯”ä¾‹ç‰¹å¾
        if 'GC_content' in all_data.columns and 'AT_content' in all_data.columns:
            all_data['GC_AT_ratio'] = all_data['GC_content'] / (all_data['AT_content'] + 1e-8)
            all_data['AT_GC_ratio'] = all_data['AT_content'] / (all_data['GC_content'] + 1e-8)
            all_data['GC_AT_diff'] = all_data['GC_content'] - all_data['AT_content']
            all_data['GC_AT_sum'] = all_data['GC_content'] + all_data['AT_content']
        
        # é•¿åº¦ç›¸å…³ç‰¹å¾
        if 'length' in all_data.columns:
            all_data['length_log'] = np.log1p(all_data['length'])
            all_data['length_sqrt'] = np.sqrt(all_data['length'])
            all_data['length_squared'] = all_data['length'] ** 2
            all_data['length_reciprocal'] = 1 / (all_data['length'] + 1)
        
        # èƒ½é‡ç›¸å…³ç‰¹å¾
        if 'Average free energy' in all_data.columns and 'length' in all_data.columns:
            all_data['energy_per_length'] = all_data['Average free energy'] / (all_data['length'] + 1)
            all_data['energy_density'] = abs(all_data['Average free energy']) / (all_data['length'] + 1)
            all_data['energy_log'] = np.log1p(abs(all_data['Average free energy']))
        
        # miRNAç»“åˆç›¸å…³ç‰¹å¾
        mirna_cols = [col for col in all_data.columns if 'miRNA' in col]
        if mirna_cols:
            all_data['miRNA_binding_total'] = all_data[mirna_cols].sum(axis=1)
            all_data['miRNA_binding_mean'] = all_data[mirna_cols].mean(axis=1)
            all_data['miRNA_binding_std'] = all_data[mirna_cols].std(axis=1)
            all_data['miRNA_binding_max'] = all_data[mirna_cols].max(axis=1)
            all_data['miRNA_binding_min'] = all_data[mirna_cols].min(axis=1)
            
            if 'length' in all_data.columns:
                all_data['miRNA_density'] = all_data['miRNA_binding_total'] / (all_data['length'] + 1)
        
        # 2. åºåˆ—å¤æ‚åº¦ç‰¹å¾
        print("2. åºåˆ—å¤æ‚åº¦ç‰¹å¾...")
        
        # åŸºäºç°æœ‰ç‰¹å¾è®¡ç®—åºåˆ—å¤æ‚åº¦
        if 'GC_content' in all_data.columns and 'AT_content' in all_data.columns:
            # é¦™å†œç†µè¿‘ä¼¼ (åŸºäºGC/ATå«é‡)
            gc_ratio = all_data['GC_content'] / 100.0
            at_ratio = all_data['AT_content'] / 100.0
            
            # é¿å…log(0)
            gc_ratio = np.clip(gc_ratio, 1e-8, 1-1e-8)
            at_ratio = np.clip(at_ratio, 1e-8, 1-1e-8)
            
            all_data['sequence_entropy'] = -(gc_ratio * np.log2(gc_ratio) + at_ratio * np.log2(at_ratio))
            all_data['gc_skewness'] = (all_data['GC_content'] - 50) / 50  # GCåæ–œåº¦
            all_data['at_skewness'] = (all_data['AT_content'] - 50) / 50  # ATåæ–œåº¦
        
        # 3. k-meré¢‘ç‡ç‰¹å¾ (æ¨¡æ‹Ÿ)
        print("3. k-meré¢‘ç‡ç‰¹å¾...")
        
        # åŸºäºç°æœ‰ç‰¹å¾æ¨¡æ‹Ÿk-merç‰¹å¾
        if 'GC_content' in all_data.columns:
            # æ¨¡æ‹Ÿä¸åŒk-merçš„é¢‘ç‡
            all_data['kmer_GC_rich'] = (all_data['GC_content'] / 100) ** 2
            all_data['kmer_AT_rich'] = (all_data['AT_content'] / 100) ** 2
            all_data['kmer_balanced'] = 4 * (all_data['GC_content'] / 100) * (all_data['AT_content'] / 100)
        
        # 4. äºŒçº§ç»“æ„ç‰¹å¾å¢å¼º
        print("4. äºŒçº§ç»“æ„ç‰¹å¾å¢å¼º...")
        
        if 'Average free energy' in all_data.columns:
            # ç»“æ„ç¨³å®šæ€§æŒ‡æ ‡
            all_data['structure_stability'] = -all_data['Average free energy'] / (all_data['length'] + 1)
            all_data['mfe_normalized'] = all_data['Average free energy'] / np.sqrt(all_data['length'] + 1)
            
            # åŸºäºMFEçš„åˆ†ç±»ç‰¹å¾
            all_data['mfe_category'] = pd.cut(all_data['Average free energy'], bins=5, labels=['very_stable', 'stable', 'moderate', 'unstable', 'very_unstable'])
        
        # 5. äº¤äº’ç‰¹å¾
        print("5. äº¤äº’ç‰¹å¾...")
        
        # é‡è¦ç‰¹å¾çš„äº¤äº’
        important_features = ['GC_content', 'AT_content', 'length', 'Average free energy']
        available_features = [f for f in important_features if f in all_data.columns]
        
        for i, feat1 in enumerate(available_features):
            for feat2 in available_features[i+1:]:
                all_data[f'{feat1}_x_{feat2}'] = all_data[feat1] * all_data[feat2]
                all_data[f'{feat1}_div_{feat2}'] = all_data[feat1] / (all_data[feat2] + 1e-8)
        
        # 6. ç»Ÿè®¡ç‰¹å¾
        print("6. ç»Ÿè®¡ç‰¹å¾...")
        
        # æ•°å€¼ç‰¹å¾çš„ç»Ÿè®¡å˜æ¢
        numeric_features = all_data.select_dtypes(include=[np.number]).columns.tolist()
        if 'ID' in numeric_features:
            numeric_features.remove('ID')
        
        # æ·»åŠ å¤šé¡¹å¼ç‰¹å¾ (é€‰æ‹©æ€§)
        key_features = ['GC_content', 'AT_content', 'length'][:3]  # é™åˆ¶ç‰¹å¾æ•°é‡
        available_key_features = [f for f in key_features if f in all_data.columns]
        
        if len(available_key_features) >= 2:
            poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
            poly_features = poly.fit_transform(all_data[available_key_features])
            poly_feature_names = poly.get_feature_names_out(available_key_features)
            
            # åªæ·»åŠ äº¤äº’é¡¹ï¼Œé¿å…é‡å¤
            for i, name in enumerate(poly_feature_names):
                if ' ' in name:  # äº¤äº’é¡¹åŒ…å«ç©ºæ ¼
                    all_data[f'poly_{name.replace(" ", "_")}'] = poly_features[:, i]
        
        # 7. èšç±»ç‰¹å¾
        print("7. èšç±»ç‰¹å¾...")
        
        # åŸºäºä¸»è¦ç‰¹å¾è¿›è¡Œèšç±»
        cluster_features = ['GC_content', 'AT_content', 'length']
        available_cluster_features = [f for f in cluster_features if f in all_data.columns]
        
        if len(available_cluster_features) >= 2:
            # å¡«å……ç¼ºå¤±å€¼
            cluster_data = all_data[available_cluster_features].fillna(all_data[available_cluster_features].median())
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            cluster_data_scaled = scaler.fit_transform(cluster_data)
            
            # K-meansèšç±»
            for n_clusters in [3, 5, 8]:
                kmeans = KMeans(n_clusters=n_clusters, random_state=self.random_state, n_init=10)
                all_data[f'cluster_{n_clusters}'] = kmeans.fit_predict(cluster_data_scaled)
        
        # 8. è·ç¦»ç‰¹å¾
        print("8. è·ç¦»ç‰¹å¾...")
        
        if len(available_cluster_features) >= 2:
            # è®¡ç®—åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»
            cluster_data = all_data[available_cluster_features].fillna(all_data[available_cluster_features].median())
            scaler = StandardScaler()
            cluster_data_scaled = scaler.fit_transform(cluster_data)
            
            # åˆ°æœ€è¿‘é‚»çš„è·ç¦»
            nn = NearestNeighbors(n_neighbors=5)
            nn.fit(cluster_data_scaled)
            distances, _ = nn.kneighbors(cluster_data_scaled)
            
            all_data['nn_distance_mean'] = distances.mean(axis=1)
            all_data['nn_distance_std'] = distances.std(axis=1)
            all_data['nn_distance_min'] = distances.min(axis=1)
        
        # å¤„ç†åˆ†ç±»ç‰¹å¾
        categorical_features = all_data.select_dtypes(include=['object']).columns.tolist()
        if 'ID' in categorical_features:
            categorical_features.remove('ID')
        
        print(f"\nå¤„ç† {len(categorical_features)} ä¸ªåˆ†ç±»ç‰¹å¾...")
        
        for feature in categorical_features:
            if feature not in self.encoders:
                self.encoders[feature] = LabelEncoder()
                all_data[feature] = self.encoders[feature].fit_transform(all_data[feature].astype(str))
            else:
                all_data[feature] = self.encoders[feature].transform(all_data[feature].astype(str))
        
        # åˆ†ç¦»è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        train_size = len(self.train_data)
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        feature_columns = [col for col in all_data.columns if col != 'ID']
        self.X_train = all_data.iloc[:train_size][feature_columns]
        self.X_test = all_data.iloc[train_size:][feature_columns]
        
        # ç›®æ ‡å˜é‡ç¼–ç 
        self.y_train = self.target_encoder.fit_transform(self.train_data['Tissue'])
        
        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼ç±»å‹
        for col in feature_columns:
            # å¤„ç†Categoricalç±»å‹
            if hasattr(self.X_train[col], 'cat'):
                self.X_train[col] = self.X_train[col].astype(str)
                self.X_test[col] = self.X_test[col].astype(str)
            
            # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            if self.X_train[col].dtype == 'object':
                self.X_train[col] = pd.to_numeric(self.X_train[col], errors='coerce')
                self.X_test[col] = pd.to_numeric(self.X_test[col], errors='coerce')
        
        # å¤„ç†è½¬æ¢åå¯èƒ½äº§ç”Ÿçš„NaNå€¼
        self.X_train = self.X_train.fillna(0)
        self.X_test = self.X_test.fillna(0)
        
        # ä¿å­˜ç‰¹å¾åç§°
        self.feature_names = feature_columns
        self.engineered_features = [col for col in feature_columns if col not in self.original_features]
        
        print(f"\nç‰¹å¾å·¥ç¨‹å®Œæˆ:")
        print(f"  åŸå§‹ç‰¹å¾: {len(self.original_features)}")
        print(f"  æ–°å¢ç‰¹å¾: {len(self.engineered_features)}")
        print(f"  æ€»ç‰¹å¾æ•°: {len(self.feature_names)}")
        print(f"  æ•°æ®ç±»å‹æ£€æŸ¥: {self.X_train.dtypes.value_counts().to_dict()}")
        
        return self
    
    def advanced_preprocessing(self):
        """
        é«˜çº§æ•°æ®é¢„å¤„ç†
        """
        print("\né«˜çº§æ•°æ®é¢„å¤„ç†...")
        print("=" * 50)
        
        # 1. å¤„ç†ç¼ºå¤±å€¼
        print("1. å¤„ç†ç¼ºå¤±å€¼...")
        
        # ä½¿ç”¨ä¸­ä½æ•°å¡«å……æ•°å€¼ç‰¹å¾
        numeric_features = self.X_train.select_dtypes(include=[np.number]).columns
        for feature in numeric_features:
            median_value = self.X_train[feature].median()
            self.X_train[feature].fillna(median_value, inplace=True)
            self.X_test[feature].fillna(median_value, inplace=True)
        
        # 2. å¼‚å¸¸å€¼å¤„ç†
        print("2. å¼‚å¸¸å€¼å¤„ç†...")
        
        # ä½¿ç”¨IQRæ–¹æ³•å¤„ç†å¼‚å¸¸å€¼
        for feature in numeric_features:
            Q1 = self.X_train[feature].quantile(0.25)
            Q3 = self.X_train[feature].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # è£å‰ªå¼‚å¸¸å€¼
            self.X_train[feature] = np.clip(self.X_train[feature], lower_bound, upper_bound)
            self.X_test[feature] = np.clip(self.X_test[feature], lower_bound, upper_bound)
        
        # 3. ç‰¹å¾ç¼©æ”¾
        print("3. ç‰¹å¾ç¼©æ”¾...")
        
        # åªå¯¹æ•°å€¼ç‰¹å¾è¿›è¡Œç¼©æ”¾ï¼Œä¿æŒåˆ†ç±»ç‰¹å¾ä¸å˜
        numeric_features = self.X_train.select_dtypes(include=[np.number]).columns
        categorical_features = self.X_train.select_dtypes(exclude=[np.number]).columns
        
        if len(numeric_features) > 0:
            # ä½¿ç”¨RobustScaler (å¯¹å¼‚å¸¸å€¼æ›´é²æ£’)
            self.scalers['robust'] = RobustScaler()
            X_train_numeric_scaled = self.scalers['robust'].fit_transform(self.X_train[numeric_features])
            X_test_numeric_scaled = self.scalers['robust'].transform(self.X_test[numeric_features])
            
            # é‡æ–°ç»„åˆæ•°æ®
            X_train_numeric_df = pd.DataFrame(X_train_numeric_scaled, columns=numeric_features, index=self.X_train.index)
            X_test_numeric_df = pd.DataFrame(X_test_numeric_scaled, columns=numeric_features, index=self.X_test.index)
            
            # åˆå¹¶æ•°å€¼ç‰¹å¾å’Œåˆ†ç±»ç‰¹å¾
            if len(categorical_features) > 0:
                self.X_train = pd.concat([X_train_numeric_df, self.X_train[categorical_features]], axis=1)
                self.X_test = pd.concat([X_test_numeric_df, self.X_test[categorical_features]], axis=1)
            else:
                self.X_train = X_train_numeric_df
                self.X_test = X_test_numeric_df
            
            # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
            self.X_train = self.X_train[self.feature_names]
            self.X_test = self.X_test[self.feature_names]
        
        print(f"é¢„å¤„ç†å®Œæˆï¼Œç‰¹å¾å½¢çŠ¶: {self.X_train.shape}")
        
        return self
    
    def advanced_feature_selection(self):
        """
        é«˜çº§ç‰¹å¾é€‰æ‹©
        """
        print("\né«˜çº§ç‰¹å¾é€‰æ‹©...")
        print("=" * 50)
        
        original_feature_count = self.X_train.shape[1]
        
        # 1. æ–¹å·®é˜ˆå€¼è¿‡æ»¤
        print("1. æ–¹å·®é˜ˆå€¼è¿‡æ»¤...")
        variance_selector = VarianceThreshold(threshold=0.01)
        X_train_var = variance_selector.fit_transform(self.X_train)
        X_test_var = variance_selector.transform(self.X_test)
        
        selected_features_var = self.X_train.columns[variance_selector.get_support()]
        print(f"  æ–¹å·®è¿‡æ»¤åç‰¹å¾æ•°: {len(selected_features_var)}")
        
        # 2. å•å˜é‡ç‰¹å¾é€‰æ‹© (F-test)
        print("2. å•å˜é‡ç‰¹å¾é€‰æ‹©...")
        k_best = min(200, len(selected_features_var))  # é€‰æ‹©æœ€å¤š200ä¸ªç‰¹å¾
        f_selector = SelectKBest(score_func=f_classif, k=k_best)
        
        X_train_f = f_selector.fit_transform(X_train_var, self.y_train)
        X_test_f = f_selector.transform(X_test_var)
        
        selected_features_f = selected_features_var[f_selector.get_support()]
        print(f"  F-testé€‰æ‹©åç‰¹å¾æ•°: {len(selected_features_f)}")
        
        # 3. äº’ä¿¡æ¯ç‰¹å¾é€‰æ‹©
        print("3. äº’ä¿¡æ¯ç‰¹å¾é€‰æ‹©...")
        k_mutual = min(150, len(selected_features_f))  # è¿›ä¸€æ­¥ç­›é€‰
        mutual_selector = SelectKBest(score_func=mutual_info_classif, k=k_mutual)
        
        X_train_mutual = mutual_selector.fit_transform(X_train_f, self.y_train)
        X_test_mutual = mutual_selector.transform(X_test_f)
        
        selected_features_mutual = selected_features_f[mutual_selector.get_support()]
        print(f"  äº’ä¿¡æ¯é€‰æ‹©åç‰¹å¾æ•°: {len(selected_features_mutual)}")
        
        # 4. åŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹© (LASSO)
        print("4. LASSOç‰¹å¾é€‰æ‹©...")
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_weights = compute_class_weight('balanced', classes=np.unique(self.y_train), y=self.y_train)
        class_weight_dict = dict(zip(np.unique(self.y_train), class_weights))
        
        lasso = LogisticRegression(
            penalty='l1', solver='liblinear', 
            C=0.1, random_state=self.random_state,
            class_weight=class_weight_dict, max_iter=1000
        )
        
        lasso_selector = SelectFromModel(lasso, threshold='median')
        X_train_lasso = lasso_selector.fit_transform(X_train_mutual, self.y_train)
        X_test_lasso = lasso_selector.transform(X_test_mutual)
        
        selected_features_lasso = selected_features_mutual[lasso_selector.get_support()]
        print(f"  LASSOé€‰æ‹©åç‰¹å¾æ•°: {len(selected_features_lasso)}")
        
        # 5. é€’å½’ç‰¹å¾æ¶ˆé™¤ (ä½¿ç”¨éšæœºæ£®æ—)
        print("5. é€’å½’ç‰¹å¾æ¶ˆé™¤...")
        
        if len(selected_features_lasso) > 50:
            rf_estimator = RandomForestClassifier(
                n_estimators=50, random_state=self.random_state,
                class_weight='balanced', n_jobs=-1
            )
            
            rfe_selector = RFE(
                estimator=rf_estimator, 
                n_features_to_select=min(50, len(selected_features_lasso)),
                step=0.1
            )
            
            X_train_rfe = rfe_selector.fit_transform(X_train_lasso, self.y_train)
            X_test_rfe = rfe_selector.transform(X_test_lasso)
            
            selected_features_final = selected_features_lasso[rfe_selector.get_support()]
            
            # æ›´æ–°æ•°æ®
            self.X_train = pd.DataFrame(X_train_rfe, columns=selected_features_final)
            self.X_test = pd.DataFrame(X_test_rfe, columns=selected_features_final)
            
            print(f"  RFEé€‰æ‹©åç‰¹å¾æ•°: {len(selected_features_final)}")
        else:
            # å¦‚æœç‰¹å¾æ•°å·²ç»å¾ˆå°‘ï¼Œç›´æ¥ä½¿ç”¨LASSOç»“æœ
            self.X_train = pd.DataFrame(X_train_lasso, columns=selected_features_lasso)
            self.X_test = pd.DataFrame(X_test_lasso, columns=selected_features_lasso)
            selected_features_final = selected_features_lasso
        
        self.feature_names = list(selected_features_final)
        
        print(f"\nç‰¹å¾é€‰æ‹©å®Œæˆ:")
        print(f"  åŸå§‹ç‰¹å¾æ•°: {original_feature_count}")
        print(f"  æœ€ç»ˆç‰¹å¾æ•°: {len(self.feature_names)}")
        print(f"  ç‰¹å¾å‡å°‘: {original_feature_count - len(self.feature_names)} ({(1-len(self.feature_names)/original_feature_count)*100:.1f}%)")
        
        return self
    
    def create_advanced_models(self):
        """
        åˆ›å»ºé«˜çº§æ¨¡å‹é›†åˆ
        """
        print("\nåˆ›å»ºé«˜çº§æ¨¡å‹...")
        print("=" * 50)
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_weights = compute_class_weight('balanced', classes=np.unique(self.y_train), y=self.y_train)
        class_weight_dict = dict(zip(np.unique(self.y_train), class_weights))
        
        # 1. éšæœºæ£®æ— (å¢å¼ºç‰ˆ)
        self.models['RandomForest'] = RandomForestClassifier(
            n_estimators=300,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            class_weight='balanced',
            random_state=self.random_state,
            n_jobs=-1
        )
        
        # 2. Extra Trees (å¢å¼ºç‰ˆ)
        self.models['ExtraTrees'] = ExtraTreesClassifier(
            n_estimators=300,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            class_weight='balanced',
            random_state=self.random_state,
            n_jobs=-1
        )
        
        # 3. XGBoost
        if XGBOOST_AVAILABLE:
            # è½¬æ¢ç±»åˆ«æƒé‡ä¸ºXGBoostæ ¼å¼
            scale_pos_weight = len(self.y_train) / (len(np.unique(self.y_train)) * np.bincount(self.y_train))
            
            self.models['XGBoost'] = xgb.XGBClassifier(
                n_estimators=300,
                max_depth=8,
                learning_rate=0.1,
                subsample=0.9,
                colsample_bytree=0.9,
                random_state=self.random_state,
                n_jobs=-1,
                eval_metric='mlogloss'
            )
        
        # 4. LightGBM
        if LIGHTGBM_AVAILABLE:
            self.models['LightGBM'] = lgb.LGBMClassifier(
                n_estimators=300,
                max_depth=8,
                learning_rate=0.1,
                num_leaves=50,
                subsample=0.9,
                colsample_bytree=0.9,
                class_weight='balanced',
                random_state=self.random_state,
                n_jobs=-1,
                verbose=-1
            )
        
        # 5. CatBoost
        if CATBOOST_AVAILABLE:
            self.models['CatBoost'] = CatBoostClassifier(
                iterations=300,
                depth=8,
                learning_rate=0.1,
                class_weights=list(class_weights),
                random_seed=self.random_state,
                verbose=False,
                thread_count=-1
            )
        
        # 6. é€»è¾‘å›å½’ (æ­£åˆ™åŒ–)
        self.models['LogisticRegression'] = LogisticRegression(
            C=1.0,
            penalty='elasticnet',
            l1_ratio=0.5,
            solver='saga',
            class_weight='balanced',
            random_state=self.random_state,
            max_iter=2000,
            n_jobs=-1
        )
        
        # 7. Baggingåˆ†ç±»å™¨
        base_rf = RandomForestClassifier(
            n_estimators=50,
            max_depth=10,
            class_weight='balanced',
            random_state=self.random_state
        )
        
        self.models['BaggingRF'] = BaggingClassifier(
            estimator=base_rf,
            n_estimators=10,
            random_state=self.random_state,
            n_jobs=-1
        )
        
        print(f"åˆ›å»ºäº† {len(self.models)} ä¸ªæ¨¡å‹")
        for model_name in self.models.keys():
            print(f"  - {model_name}")
        
        return self
    
    def evaluate_models_advanced(self):
        """
        é«˜çº§æ¨¡å‹è¯„ä¼°
        """
        print("\né«˜çº§æ¨¡å‹è¯„ä¼°...")
        print("=" * 50)
        
        # ä½¿ç”¨åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯
        cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        results = []
        
        for model_name, model in self.models.items():
            print(f"\nè¯„ä¼° {model_name}...")
            
            try:
                start_time = time.time()
                
                # äº¤å‰éªŒè¯
                cv_scores = cross_val_score(
                    model, self.X_train, self.y_train,
                    cv=cv_strategy, scoring='f1_macro', n_jobs=-1
                )
                
                training_time = time.time() - start_time
                
                result = {
                    'model_name': model_name,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std(),
                    'cv_scores': cv_scores,
                    'training_time': training_time
                }
                
                results.append(result)
                
                print(f"  CV Macro-F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
                print(f"  è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
                
            except Exception as e:
                print(f"  é”™è¯¯: {str(e)}")
        
        # æ’åºç»“æœ
        results.sort(key=lambda x: x['cv_mean'], reverse=True)
        
        print("\næ¨¡å‹æ€§èƒ½æ’è¡Œ:")
        print("-" * 60)
        for i, result in enumerate(results, 1):
            print(f"{i}. {result['model_name']:15s} - CV: {result['cv_mean']:.4f} (+/- {result['cv_std']*2:.4f})")
        
        self.model_results = results
        return self
    
    def optimize_hyperparameters_advanced(self, top_k=3):
        """
        é«˜çº§è¶…å‚æ•°ä¼˜åŒ–
        """
        print(f"\né«˜çº§è¶…å‚æ•°ä¼˜åŒ– (Top {top_k} æ¨¡å‹)...")
        print("=" * 50)
        
        # è·å–æœ€å¥½çš„å‡ ä¸ªæ¨¡å‹
        top_models = self.model_results[:top_k]
        optimized_models = {}
        
        # ä¼˜åŒ–çš„è¶…å‚æ•°æœç´¢ç©ºé—´ï¼ˆå‡å°‘å¤æ‚åº¦æé«˜æ•ˆç‡ï¼‰
        param_grids = {
            'RandomForest': {
                'n_estimators': [200, 300, 500],
                'max_depth': [10, 15, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', 0.8],
                'bootstrap': [True, False]
            },
            'ExtraTrees': {
                'n_estimators': [200, 300, 500],
                'max_depth': [10, 15, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', 0.8],
                'bootstrap': [True, False]
            },
            'XGBoost': {
                'n_estimators': [200, 300, 500],
                'max_depth': [6, 8, 10],
                'learning_rate': [0.05, 0.1, 0.15],
                'subsample': [0.8, 0.9],
                'colsample_bytree': [0.8, 0.9],
                'gamma': [0, 0.1],
                'reg_alpha': [0, 0.1],
                'reg_lambda': [1, 1.5]
            },
            'LightGBM': {
                'n_estimators': [200, 300, 500],
                'max_depth': [6, 8, 10],
                'learning_rate': [0.05, 0.1, 0.15],
                'num_leaves': [31, 50, 100],
                'subsample': [0.8, 0.9],
                'colsample_bytree': [0.8, 0.9],
                'reg_alpha': [0, 0.1],
                'reg_lambda': [0, 0.1]
            }
        }
        
        if CATBOOST_AVAILABLE:
            param_grids['CatBoost'] = {
                'iterations': [200, 300, 500],
                'depth': [6, 8, 10],
                'learning_rate': [0.05, 0.1, 0.15],
                'l2_leaf_reg': [1, 3, 5],
                'border_count': [32, 64],
                'bagging_temperature': [0, 0.5]
            }
        
        cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)
        
        for model_result in top_models:
            model_name = model_result['model_name']
            
            if model_name not in param_grids:
                print(f"è·³è¿‡ {model_name} (æ— è¶…å‚æ•°ç½‘æ ¼)")
                optimized_models[model_name] = self.models[model_name]
                continue
            
            print(f"\nä¼˜åŒ– {model_name}...")
            
            base_model = self.models[model_name]
            param_grid = param_grids[model_name]
            
            # ä½¿ç”¨RandomizedSearchCVè¿›è¡Œé«˜æ•ˆæœç´¢
            search = RandomizedSearchCV(
                estimator=base_model,
                param_distributions=param_grid,
                n_iter=20,  # ä¼˜åŒ–æœç´¢æ¬¡æ•°ä»¥æé«˜æ•ˆç‡
                cv=cv_strategy,
                scoring='f1_macro',
                n_jobs=-1,
                random_state=self.random_state,
                verbose=0
            )
            
            start_time = time.time()
            search.fit(self.X_train, self.y_train)
            search_time = time.time() - start_time
            
            optimized_models[model_name] = search.best_estimator_
            
            print(f"  æœ€ä½³CVåˆ†æ•°: {search.best_score_:.4f}")
            print(f"  ä¼˜åŒ–æ—¶é—´: {search_time:.2f}ç§’")
            print(f"  æœ€ä½³å‚æ•°: {search.best_params_}")
        
        self.optimized_models = optimized_models
        return self
    
    def create_advanced_ensemble(self):
        """
        åˆ›å»ºé«˜çº§é›†æˆæ¨¡å‹
        """
        print("\nåˆ›å»ºé«˜çº§é›†æˆæ¨¡å‹...")
        print("=" * 50)
        
        # å‡†å¤‡åŸºæ¨¡å‹
        base_models = []
        for name, model in self.optimized_models.items():
            base_models.append((name, model))
        
        print(f"é›†æˆ {len(base_models)} ä¸ªåŸºæ¨¡å‹")
        
        # 1. Voting Classifier (è½¯æŠ•ç¥¨)
        voting_clf = VotingClassifier(
            estimators=base_models,
            voting='soft',
            n_jobs=-1
        )
        
        # 2. Stacking Classifier (å¤šå±‚)
        # ç¬¬ä¸€å±‚å…ƒå­¦ä¹ å™¨
        meta_learner_1 = LogisticRegression(
            random_state=self.random_state,
            max_iter=2000,
            class_weight='balanced',
            C=1.0
        )
        
        stacking_clf = StackingClassifier(
            estimators=base_models,
            final_estimator=meta_learner_1,
            cv=5,
            n_jobs=-1,
            passthrough=True  # ä¼ é€’åŸå§‹ç‰¹å¾
        )
        
        # 3. åŠ æƒé›†æˆ (åŸºäºCVæ€§èƒ½)
        weights = []
        for model_result in self.model_results:
            if model_result['model_name'] in self.optimized_models:
                weights.append(model_result['cv_mean'])
        
        # å½’ä¸€åŒ–æƒé‡
        weights = np.array(weights)
        weights = weights / weights.sum()
        
        weighted_voting_clf = VotingClassifier(
            estimators=base_models,
            voting='soft',
            weights=weights,
            n_jobs=-1
        )
        
        # è¯„ä¼°é›†æˆæ¨¡å‹
        cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        print("\nè¯„ä¼°é›†æˆæ¨¡å‹...")
        
        ensemble_results = {}
        
        # è¯„ä¼°Voting
        voting_scores = cross_val_score(
            voting_clf, self.X_train, self.y_train,
            cv=cv_strategy, scoring='f1_macro', n_jobs=-1
        )
        ensemble_results['Voting'] = voting_scores.mean()
        print(f"Voting Ensemble CV: {voting_scores.mean():.4f} (+/- {voting_scores.std()*2:.4f})")
        
        # è¯„ä¼°åŠ æƒVoting
        weighted_voting_scores = cross_val_score(
            weighted_voting_clf, self.X_train, self.y_train,
            cv=cv_strategy, scoring='f1_macro', n_jobs=-1
        )
        ensemble_results['WeightedVoting'] = weighted_voting_scores.mean()
        print(f"Weighted Voting CV: {weighted_voting_scores.mean():.4f} (+/- {weighted_voting_scores.std()*2:.4f})")
        
        # è¯„ä¼°Stacking
        stacking_scores = cross_val_score(
            stacking_clf, self.X_train, self.y_train,
            cv=cv_strategy, scoring='f1_macro', n_jobs=-1
        )
        ensemble_results['Stacking'] = stacking_scores.mean()
        print(f"Stacking Ensemble CV: {stacking_scores.mean():.4f} (+/- {stacking_scores.std()*2:.4f})")
        
        # é€‰æ‹©æœ€ä½³é›†æˆæ–¹æ³•
        best_ensemble_name = max(ensemble_results, key=ensemble_results.get)
        best_ensemble_score = ensemble_results[best_ensemble_name]
        
        if best_ensemble_name == 'Voting':
            self.best_ensemble = voting_clf
        elif best_ensemble_name == 'WeightedVoting':
            self.best_ensemble = weighted_voting_clf
        else:
            self.best_ensemble = stacking_clf
        
        self.best_ensemble_name = best_ensemble_name
        self.best_ensemble_score = best_ensemble_score
        
        print(f"\né€‰æ‹©æœ€ä½³é›†æˆ: {best_ensemble_name} (CV: {best_ensemble_score:.4f})")
        
        # æ¯”è¾ƒå•æ¨¡å‹å’Œé›†æˆæ¨¡å‹
        best_single_score = max(result['cv_mean'] for result in self.model_results)
        improvement = best_ensemble_score - best_single_score
        
        print(f"ç›¸æ¯”æœ€ä½³å•æ¨¡å‹æå‡: {improvement:.4f} ({improvement/best_single_score*100:.2f}%)")
        
        return self
    
    def train_final_model_advanced(self):
        """
        è®­ç»ƒæœ€ç»ˆæ¨¡å‹ (å¸¦æ ¡å‡†)
        """
        print("\nè®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        print("=" * 50)
        
        # é€‰æ‹©æœ€ç»ˆæ¨¡å‹
        best_single_score = max(result['cv_mean'] for result in self.model_results)
        
        if hasattr(self, 'best_ensemble') and self.best_ensemble_score > best_single_score:
            base_model = self.best_ensemble
            final_model_name = f'Ensemble ({self.best_ensemble_name})'
            final_score = self.best_ensemble_score
        else:
            # ä½¿ç”¨æœ€ä½³å•æ¨¡å‹
            best_model_name = self.model_results[0]['model_name']
            base_model = self.optimized_models[best_model_name]
            final_model_name = f'{best_model_name} (Optimized)'
            final_score = self.model_results[0]['cv_mean']
        
        print(f"åŸºç¡€æ¨¡å‹: {final_model_name}")
        print(f"é¢„æœŸCVåˆ†æ•°: {final_score:.4f}")
        
        # æ¨¡å‹æ ¡å‡†
        print("\nåº”ç”¨æ¨¡å‹æ ¡å‡†...")
        calibrated_model = CalibratedClassifierCV(
            base_model, 
            method='isotonic',  # æˆ– 'sigmoid'
            cv=3
        )
        
        # åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ
        print("åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ...")
        start_time = time.time()
        calibrated_model.fit(self.X_train, self.y_train)
        training_time = time.time() - start_time
        
        print(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
        
        self.final_model = calibrated_model
        self.final_model_name = f'{final_model_name} (Calibrated)'
        self.final_score = final_score
        
        return self
    
    def generate_predictions_advanced(self, output_file='submit_v2.csv'):
        """
        ç”Ÿæˆé«˜çº§é¢„æµ‹ç»“æœ
        """
        print("\nç”Ÿæˆé¢„æµ‹ç»“æœ...")
        print("=" * 50)
        
        # é¢„æµ‹æ¦‚ç‡
        print("é¢„æµ‹æµ‹è¯•é›†æ¦‚ç‡...")
        y_pred_proba = self.final_model.predict_proba(self.X_test)
        
        # åŸºç¡€é¢„æµ‹
        y_pred_encoded = self.final_model.predict(self.X_test)
        
        # é˜ˆå€¼ä¼˜åŒ– (å¯é€‰)
        # è¿™é‡Œå¯ä»¥æ ¹æ®éªŒè¯é›†ç»“æœè°ƒæ•´é¢„æµ‹é˜ˆå€¼
        
        # è§£ç é¢„æµ‹ç»“æœ
        y_pred = self.target_encoder.inverse_transform(y_pred_encoded)
        
        # åˆ›å»ºæäº¤æ–‡ä»¶
        submission = pd.DataFrame({
            'ID': self.test_ids,
            'Tissue': y_pred
        })
        
        # ä¿å­˜
        submission.to_csv(output_file, index=False)
        print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # é¢„æµ‹åˆ†å¸ƒ
        pred_dist = pd.Series(y_pred).value_counts().sort_index()
        print(f"\né¢„æµ‹åˆ†å¸ƒ:")
        for tissue, count in pred_dist.items():
            print(f"  {tissue}: {count} ({count/len(y_pred)*100:.1f}%)")
        
        # é¢„æµ‹ç½®ä¿¡åº¦åˆ†æ
        max_proba = y_pred_proba.max(axis=1)
        print(f"\né¢„æµ‹ç½®ä¿¡åº¦:")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {max_proba.mean():.3f}")
        print(f"  ç½®ä¿¡åº¦æ ‡å‡†å·®: {max_proba.std():.3f}")
        print(f"  ä½ç½®ä¿¡åº¦æ ·æœ¬ (<0.5): {(max_proba < 0.5).sum()} ({(max_proba < 0.5).mean()*100:.1f}%)")
        
        self.submission = submission
        return self
    
    def save_model_advanced(self, model_file='final_model_v2.pkl'):
        """
        ä¿å­˜é«˜çº§æ¨¡å‹
        """
        print(f"\nä¿å­˜æ¨¡å‹åˆ°: {model_file}")
        
        model_data = {
            'model': self.final_model,
            'model_name': self.final_model_name,
            'cv_score': self.final_score,
            'encoders': self.encoders,
            'scalers': self.scalers,
            'feature_names': self.feature_names,
            'original_features': self.original_features,
            'engineered_features': self.engineered_features,
            'target_encoder': self.target_encoder,
            'model_results': self.model_results,
            'training_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'pipeline_version': '2.0'
        }
        
        with open(model_file, 'wb') as f:
            pickle.dump(model_data, f)
        
        print("æ¨¡å‹ä¿å­˜å®Œæˆ")
        
        return self
    
    def run_complete_pipeline_advanced(self, train_path='train.csv', test_path='test.csv', 
                                     output_file='submit_v2.csv', model_file='final_model_v2.pkl'):
        """
        è¿è¡Œå®Œæ•´çš„é«˜çº§é¢„æµ‹pipeline
        """
        print("=" * 80)
        print("        ç¯çŠ¶RNAç»„ç»‡è¡¨è¾¾é¢„æµ‹ - é«˜çº§Pipeline v2.0")
        print("=" * 80)
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œå®Œæ•´æµç¨‹
            (self.load_data(train_path, test_path)
             .analyze_data()
             .advanced_feature_engineering()
             .advanced_preprocessing()
             .advanced_feature_selection()
             .create_advanced_models()
             .evaluate_models_advanced()
             .optimize_hyperparameters_advanced()
             .create_advanced_ensemble()
             .train_final_model_advanced()
             .generate_predictions_advanced(output_file)
             .save_model_advanced(model_file))
            
            total_time = time.time() - start_time
            
            print("\n" + "=" * 80)
            print("                    é«˜çº§Pipeline v2.0 å®Œæˆ")
            print("=" * 80)
            print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
            print(f"æœ€ç»ˆæ¨¡å‹: {self.final_model_name}")
            print(f"é¢„æœŸæ€§èƒ½: {self.final_score:.4f} (Macro-F1)")
            print(f"ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            print(f"é¢„æµ‹æ–‡ä»¶: {output_file}")
            print(f"æ¨¡å‹æ–‡ä»¶: {model_file}")
            print("=" * 80)
            
            return True
            
        except Exception as e:
            print(f"\nPipelineæ‰§è¡Œå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """
    ä¸»å‡½æ•°
    """
    # åˆ›å»ºé«˜çº§pipelineå®ä¾‹
    pipeline = AdvancedRNAPredictionPipeline(random_state=42)
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    success = pipeline.run_complete_pipeline_advanced(
        train_path='train.csv',
        test_path='test.csv',
        output_file='submit_v2.csv',
        model_file='final_model_v2.pkl'
    )
    
    if success:
        print("\nğŸ‰ é«˜çº§é¢„æµ‹ä»»åŠ¡å®Œæˆï¼")
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   - submit_v2.csv: æ”¹è¿›ç‰ˆæäº¤æ–‡ä»¶")
        print("   - final_model_v2.pkl: æ”¹è¿›ç‰ˆè®­ç»ƒæ¨¡å‹")
        print("\nğŸš€ ä¸»è¦æ”¹è¿›:")
        print("   - æ›´å¤šç”Ÿç‰©å­¦ç‰¹å¾å·¥ç¨‹")
        print("   - é«˜çº§ç‰¹å¾é€‰æ‹©ç­–ç•¥")
        print("   - ç²¾ç»†è¶…å‚æ•°ä¼˜åŒ–")
        print("   - å¤šå±‚é›†æˆå’Œæ¨¡å‹æ ¡å‡†")
        print("\nğŸ’¡ æç¤º: å¦‚æœæ€§èƒ½ä»éœ€æå‡ï¼Œå¯ä»¥è€ƒè™‘:")
        print("   - æ·»åŠ æ›´å¤šé¢†åŸŸç‰¹å®šç‰¹å¾")
        print("   - å°è¯•æ·±åº¦å­¦ä¹ æ–¹æ³•")
        print("   - è¿›è¡Œæ•°æ®å¢å¼º")
    else:
        print("\nâŒ é«˜çº§é¢„æµ‹ä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()