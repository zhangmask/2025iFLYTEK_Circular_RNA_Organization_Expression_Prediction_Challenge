#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¯çŠ¶RNAç»„ç»‡è¡¨è¾¾é¢„æµ‹æŒ‘æˆ˜èµ› - å®Œæ•´è§£å†³æ–¹æ¡ˆ
ç›®æ ‡ï¼šè·å¾—å°½å¯èƒ½é«˜çš„Macro-F1åˆ†æ•°

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025å¹´
"""

import pandas as pd
import numpy as np
import warnings
import pickle
import time
from datetime import datetime
import os
from typing import Dict, List, Tuple, Any

# æœºå™¨å­¦ä¹ åº“
from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline

# æ¢¯åº¦æå‡åº“
import xgboost as xgb
import lightgbm as lgb
try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    print("CatBoost not available, will skip CatBoost models")
    CATBOOST_AVAILABLE = False

# è¶…å‚æ•°ä¼˜åŒ–
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from scipy.stats import randint, uniform

# æ¨¡å‹é›†æˆ
from sklearn.ensemble import StackingClassifier

# è®¾ç½®
warnings.filterwarnings('ignore')
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

class RNAPredictionPipeline:
    """
    ç¯çŠ¶RNAç»„ç»‡è¡¨è¾¾é¢„æµ‹å®Œæ•´pipeline
    """
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.encoders = {}
        self.scalers = {}
        self.feature_selector = None
        self.models = {}
        self.best_model = None
        self.feature_names = []
        self.target_encoder = None
        
    def load_data(self, train_path='train.csv', test_path='test.csv'):
        """
        åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        """
        print("åŠ è½½æ•°æ®...")
        self.train_df = pd.read_csv(train_path)
        self.test_df = pd.read_csv(test_path)
        
        print(f"è®­ç»ƒé›†å½¢çŠ¶: {self.train_df.shape}")
        print(f"æµ‹è¯•é›†å½¢çŠ¶: {self.test_df.shape}")
        
        # ä¿å­˜æµ‹è¯•é›†ID
        self.test_ids = self.test_df['ID'].copy()
        
        return self
    
    def analyze_data(self):
        """
        æ•°æ®åˆ†æå’Œè´¨é‡æ£€æŸ¥
        """
        print("\næ•°æ®åˆ†æ...")
        print("=" * 50)
        
        # åŸºæœ¬ä¿¡æ¯
        print("è®­ç»ƒé›†åŸºæœ¬ä¿¡æ¯:")
        print(self.train_df.info())
        
        # ç¼ºå¤±å€¼æ£€æŸ¥
        missing_train = self.train_df.isnull().sum()
        missing_test = self.test_df.isnull().sum()
        
        if missing_train.sum() > 0:
            print(f"\nè®­ç»ƒé›†ç¼ºå¤±å€¼:\n{missing_train[missing_train > 0]}")
        else:
            print("\nè®­ç»ƒé›†æ— ç¼ºå¤±å€¼")
            
        if missing_test.sum() > 0:
            print(f"\næµ‹è¯•é›†ç¼ºå¤±å€¼:\n{missing_test[missing_test > 0]}")
        else:
            print("æµ‹è¯•é›†æ— ç¼ºå¤±å€¼")
        
        # ç›®æ ‡å˜é‡åˆ†å¸ƒ
        target_dist = self.train_df['Tissue'].value_counts().sort_index()
        print(f"\nç›®æ ‡å˜é‡åˆ†å¸ƒ:")
        for tissue, count in target_dist.items():
            print(f"  {tissue}: {count} ({count/len(self.train_df)*100:.1f}%)")
        
        # ç‰¹å¾ç±»å‹åˆ†æ
        numeric_features = self.train_df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_features = self.train_df.select_dtypes(include=['object']).columns.tolist()
        
        # ç§»é™¤IDå’Œç›®æ ‡å˜é‡
        if 'ID' in numeric_features:
            numeric_features.remove('ID')
        if 'Tissue' in categorical_features:
            categorical_features.remove('Tissue')
            
        print(f"\næ•°å€¼ç‰¹å¾ ({len(numeric_features)}): {numeric_features}")
        print(f"åˆ†ç±»ç‰¹å¾ ({len(categorical_features)}): {categorical_features}")
        
        self.numeric_features = numeric_features
        self.categorical_features = categorical_features
        
        return self
    
    def feature_engineering(self):
        """
        ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®é¢„å¤„ç†
        """
        print("\nç‰¹å¾å·¥ç¨‹...")
        print("=" * 50)
        
        # å¤åˆ¶æ•°æ®
        train_processed = self.train_df.copy()
        test_processed = self.test_df.copy()
        
        # 1. å¤„ç†åˆ†ç±»ç‰¹å¾
        for feature in self.categorical_features:
            if feature in train_processed.columns:
                # ä½¿ç”¨LabelEncoder
                le = LabelEncoder()
                
                # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„å”¯ä¸€å€¼
                all_values = pd.concat([
                    train_processed[feature], 
                    test_processed[feature]
                ]).unique()
                
                le.fit(all_values)
                
                train_processed[feature] = le.transform(train_processed[feature])
                test_processed[feature] = le.transform(test_processed[feature])
                
                self.encoders[feature] = le
                print(f"ç¼–ç ç‰¹å¾ {feature}: {len(le.classes_)} ä¸ªç±»åˆ«")
        
        # 2. åˆ›å»ºæ–°ç‰¹å¾
        print("\nåˆ›å»ºæ–°ç‰¹å¾...")
        
        # æ ¸è‹·é…¸æ¯”ä¾‹ç›¸å…³ç‰¹å¾
        for df in [train_processed, test_processed]:
            # GCå«é‡ç›¸å…³
            df['GC_AT_ratio'] = df['GC_content'] / (df['A_ratio'] + df['T_ratio'] + 1e-8)
            df['GC_content_squared'] = df['GC_content'] ** 2
            
            # é•¿åº¦ç›¸å…³
            df['length_log'] = np.log1p(df['length'])
            df['length_sqrt'] = np.sqrt(df['length'])
            
            # èƒ½é‡ç›¸å…³
            df['energy_per_length'] = df['Average free energy'] / (df['length'] + 1e-8)
            df['energy_squared'] = df['Average free energy'] ** 2
            
            # miRNAç»“åˆç›¸å…³
            df['mirna_per_length'] = df['miRNA Binding count'] / (df['length'] + 1e-8)
            df['mirna_log'] = np.log1p(df['miRNA Binding count'])
            
            # æ ¸è‹·é…¸æ¯”ä¾‹äº¤äº’
            df['AT_content'] = df['A_ratio'] + df['T_ratio']
            df['purine_content'] = df['A_ratio'] + df['G_ratio']  # å˜Œå‘¤
            df['pyrimidine_content'] = df['T_ratio'] + df['C_ratio']  # å˜§å•¶
            df['purine_pyrimidine_ratio'] = df['purine_content'] / (df['pyrimidine_content'] + 1e-8)
            
            # å¤åˆç‰¹å¾
            df['complexity_score'] = (df['GC_content'] * df['length'] * df['miRNA Binding count']) / 1000
            df['stability_score'] = df['Average free energy'] * df['GC_content']
            
        print(f"åˆ›å»ºäº† {len([c for c in train_processed.columns if c not in self.train_df.columns])} ä¸ªæ–°ç‰¹å¾")
        
        # 3. å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
        # ç§»é™¤IDå’Œç›®æ ‡å˜é‡
        feature_columns = [col for col in train_processed.columns if col not in ['ID', 'Tissue']]
        
        X_train = train_processed[feature_columns]
        y_train = train_processed['Tissue']
        X_test = test_processed[feature_columns]
        
        # 4. å¤„ç†ç¼ºå¤±å€¼
        print("\nå¤„ç†ç¼ºå¤±å€¼...")
        from sklearn.impute import SimpleImputer
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_train = X_train.isnull().sum()
        missing_test = X_test.isnull().sum()
        
        if missing_train.sum() > 0 or missing_test.sum() > 0:
            print(f"è®­ç»ƒé›†ç¼ºå¤±å€¼: {missing_train.sum()}")
            print(f"æµ‹è¯•é›†ç¼ºå¤±å€¼: {missing_test.sum()}")
            
            # ä½¿ç”¨ä¸­ä½æ•°å¡«å……æ•°å€¼ç‰¹å¾
            imputer = SimpleImputer(strategy='median')
            X_train_imputed = imputer.fit_transform(X_train)
            X_test_imputed = imputer.transform(X_test)
            
            # è½¬æ¢å›DataFrame
            X_train = pd.DataFrame(X_train_imputed, columns=feature_columns)
            X_test = pd.DataFrame(X_test_imputed, columns=feature_columns)
            
            self.scalers['imputer'] = imputer
            print("ç¼ºå¤±å€¼å¤„ç†å®Œæˆ")
        else:
            print("æ— ç¼ºå¤±å€¼")
        
        # 5. ç›®æ ‡å˜é‡ç¼–ç 
        self.target_encoder = LabelEncoder()
        y_train_encoded = self.target_encoder.fit_transform(y_train)
        
        print(f"ç›®æ ‡ç±»åˆ«: {self.target_encoder.classes_}")
        
        # 6. ç‰¹å¾æ ‡å‡†åŒ–
        print("\nç‰¹å¾æ ‡å‡†åŒ–...")
        scaler = RobustScaler()  # å¯¹å¼‚å¸¸å€¼æ›´é²æ£’
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # è½¬æ¢å›DataFrame
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns)
        
        self.scalers['features'] = scaler
        
        # 7. ç‰¹å¾é€‰æ‹©
        print("\nç‰¹å¾é€‰æ‹©...")
        
        # ä½¿ç”¨å¤šç§ç‰¹å¾é€‰æ‹©æ–¹æ³•
        # æ–¹æ³•1: åŸºäºFç»Ÿè®¡é‡
        selector_f = SelectKBest(score_func=f_classif, k=min(50, len(feature_columns)))
        X_train_selected_f = selector_f.fit_transform(X_train_scaled, y_train_encoded)
        
        # æ–¹æ³•2: åŸºäºäº’ä¿¡æ¯
        selector_mi = SelectKBest(score_func=mutual_info_classif, k=min(50, len(feature_columns)))
        X_train_selected_mi = selector_mi.fit_transform(X_train_scaled, y_train_encoded)
        
        # åˆå¹¶é€‰æ‹©çš„ç‰¹å¾
        selected_features_f = set(np.array(feature_columns)[selector_f.get_support()])
        selected_features_mi = set(np.array(feature_columns)[selector_mi.get_support()])
        selected_features = list(selected_features_f.union(selected_features_mi))
        
        print(f"Fç»Ÿè®¡é‡é€‰æ‹©: {len(selected_features_f)} ä¸ªç‰¹å¾")
        print(f"äº’ä¿¡æ¯é€‰æ‹©: {len(selected_features_mi)} ä¸ªç‰¹å¾")
        print(f"åˆå¹¶å: {len(selected_features)} ä¸ªç‰¹å¾")
        
        # åº”ç”¨ç‰¹å¾é€‰æ‹©
        X_train_final = X_train_scaled[selected_features]
        X_test_final = X_test_scaled[selected_features]
        
        self.feature_names = selected_features
        self.feature_selector = {'f_selector': selector_f, 'mi_selector': selector_mi}
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        self.X_train = X_train_final
        self.y_train = y_train_encoded
        self.X_test = X_test_final
        
        print(f"\næœ€ç»ˆç‰¹å¾ç»´åº¦: {self.X_train.shape}")
        print(f"æµ‹è¯•é›†ç‰¹å¾ç»´åº¦: {self.X_test.shape}")
        
        return self
    
    def create_models(self):
        """
        åˆ›å»ºå¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹
        """
        print("\nåˆ›å»ºæ¨¡å‹...")
        print("=" * 50)
        
        models = {}
        
        # 1. Random Forest
        models['RandomForest'] = RandomForestClassifier(
            n_estimators=300,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            random_state=self.random_state,
            n_jobs=-1,
            class_weight='balanced'
        )
        
        # 2. Extra Trees
        models['ExtraTrees'] = ExtraTreesClassifier(
            n_estimators=300,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            random_state=self.random_state,
            n_jobs=-1,
            class_weight='balanced'
        )
        
        # 3. XGBoost
        models['XGBoost'] = xgb.XGBClassifier(
            n_estimators=300,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=self.random_state,
            eval_metric='mlogloss',
            verbosity=0,
            n_jobs=-1
        )
        
        # 4. LightGBM
        models['LightGBM'] = lgb.LGBMClassifier(
            n_estimators=300,
            max_depth=8,
            learning_rate=0.1,
            num_leaves=50,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=self.random_state,
            verbosity=-1,
            n_jobs=-1,
            class_weight='balanced'
        )
        
        # 5. CatBoost (å¦‚æœå¯ç”¨)
        if CATBOOST_AVAILABLE:
            models['CatBoost'] = cb.CatBoostClassifier(
                iterations=300,
                depth=8,
                learning_rate=0.1,
                random_state=self.random_state,
                verbose=False,
                thread_count=-1,
                class_weights='Balanced'
            )
        
        # 6. Logistic Regression (ä½œä¸ºåŸºçº¿)
        models['LogisticRegression'] = LogisticRegression(
            random_state=self.random_state,
            max_iter=1000,
            class_weight='balanced',
            n_jobs=-1
        )
        
        self.models = models
        print(f"åˆ›å»ºäº† {len(models)} ä¸ªæ¨¡å‹")
        
        return self
    
    def evaluate_models(self, cv_folds=5):
        """
        ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        """
        print("\næ¨¡å‹è¯„ä¼°...")
        print("=" * 50)
        
        results = []
        cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        
        for name, model in self.models.items():
            print(f"\nè¯„ä¼° {name}...")
            start_time = time.time()
            
            try:
                # äº¤å‰éªŒè¯
                cv_scores = cross_val_score(
                    model, self.X_train, self.y_train, 
                    cv=cv_strategy, scoring='f1_macro', n_jobs=-1
                )
                
                training_time = time.time() - start_time
                
                result = {
                    'model_name': name,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std(),
                    'cv_scores': cv_scores,
                    'training_time': training_time
                }
                
                results.append(result)
                
                print(f"  CV Macro-F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
                print(f"  è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
                
            except Exception as e:
                print(f"  é”™è¯¯: {str(e)}")
        
        # æ’åºç»“æœ
        results.sort(key=lambda x: x['cv_mean'], reverse=True)
        
        print("\næ¨¡å‹æ€§èƒ½æ’è¡Œ:")
        print("-" * 60)
        for i, result in enumerate(results, 1):
            print(f"{i}. {result['model_name']:15s} - CV: {result['cv_mean']:.4f} (+/- {result['cv_std']*2:.4f})")
        
        self.model_results = results
        return self
    
    def optimize_best_models(self, top_k=3):
        """
        å¯¹è¡¨ç°æœ€å¥½çš„å‡ ä¸ªæ¨¡å‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
        """
        print(f"\nè¶…å‚æ•°ä¼˜åŒ– (Top {top_k} æ¨¡å‹)...")
        print("=" * 50)
        
        # è·å–æœ€å¥½çš„å‡ ä¸ªæ¨¡å‹
        top_models = self.model_results[:top_k]
        optimized_models = {}
        
        # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
        param_grids = {
            'RandomForest': {
                'n_estimators': [200, 300, 500],
                'max_depth': [10, 15, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', 0.8]
            },
            'ExtraTrees': {
                'n_estimators': [200, 300, 500],
                'max_depth': [10, 15, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', 0.8]
            },
            'XGBoost': {
                'n_estimators': [200, 300, 500],
                'max_depth': [6, 8, 10],
                'learning_rate': [0.05, 0.1, 0.15],
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0]
            },
            'LightGBM': {
                'n_estimators': [200, 300, 500],
                'max_depth': [6, 8, 10],
                'learning_rate': [0.05, 0.1, 0.15],
                'num_leaves': [31, 50, 100],
                'subsample': [0.8, 0.9, 1.0]
            }
        }
        
        if CATBOOST_AVAILABLE:
            param_grids['CatBoost'] = {
                'iterations': [200, 300, 500],
                'depth': [6, 8, 10],
                'learning_rate': [0.05, 0.1, 0.15],
                'l2_leaf_reg': [1, 3, 5]
            }
        
        cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)
        
        for model_result in top_models:
            model_name = model_result['model_name']
            
            if model_name not in param_grids:
                print(f"è·³è¿‡ {model_name} (æ— è¶…å‚æ•°ç½‘æ ¼)")
                optimized_models[model_name] = self.models[model_name]
                continue
            
            print(f"\nä¼˜åŒ– {model_name}...")
            
            base_model = self.models[model_name]
            param_grid = param_grids[model_name]
            
            # ä½¿ç”¨RandomizedSearchCV
            search = RandomizedSearchCV(
                estimator=base_model,
                param_distributions=param_grid,
                n_iter=30,  # å‡å°‘æœç´¢æ¬¡æ•°ä»¥èŠ‚çœæ—¶é—´
                cv=cv_strategy,
                scoring='f1_macro',
                n_jobs=-1,
                random_state=self.random_state,
                verbose=0
            )
            
            start_time = time.time()
            search.fit(self.X_train, self.y_train)
            search_time = time.time() - start_time
            
            optimized_models[model_name] = search.best_estimator_
            
            print(f"  æœ€ä½³CVåˆ†æ•°: {search.best_score_:.4f}")
            print(f"  ä¼˜åŒ–æ—¶é—´: {search_time:.2f}ç§’")
            print(f"  æœ€ä½³å‚æ•°: {search.best_params_}")
        
        self.optimized_models = optimized_models
        return self
    
    def create_ensemble(self):
        """
        åˆ›å»ºé›†æˆæ¨¡å‹
        """
        print("\nåˆ›å»ºé›†æˆæ¨¡å‹...")
        print("=" * 50)
        
        # å‡†å¤‡åŸºæ¨¡å‹
        base_models = []
        for name, model in self.optimized_models.items():
            base_models.append((name, model))
        
        print(f"é›†æˆ {len(base_models)} ä¸ªåŸºæ¨¡å‹")
        
        # 1. Voting Classifier (è½¯æŠ•ç¥¨)
        voting_clf = VotingClassifier(
            estimators=base_models,
            voting='soft',
            n_jobs=-1
        )
        
        # 2. Stacking Classifier
        # ä½¿ç”¨é€»è¾‘å›å½’ä½œä¸ºå…ƒå­¦ä¹ å™¨
        meta_learner = LogisticRegression(
            random_state=self.random_state,
            max_iter=1000,
            class_weight='balanced'
        )
        
        stacking_clf = StackingClassifier(
            estimators=base_models,
            final_estimator=meta_learner,
            cv=3,
            n_jobs=-1
        )
        
        # è¯„ä¼°é›†æˆæ¨¡å‹
        cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        print("\nè¯„ä¼°é›†æˆæ¨¡å‹...")
        
        # è¯„ä¼°Voting
        voting_scores = cross_val_score(
            voting_clf, self.X_train, self.y_train,
            cv=cv_strategy, scoring='f1_macro', n_jobs=-1
        )
        
        print(f"Voting Ensemble CV: {voting_scores.mean():.4f} (+/- {voting_scores.std()*2:.4f})")
        
        # è¯„ä¼°Stacking
        stacking_scores = cross_val_score(
            stacking_clf, self.X_train, self.y_train,
            cv=cv_strategy, scoring='f1_macro', n_jobs=-1
        )
        
        print(f"Stacking Ensemble CV: {stacking_scores.mean():.4f} (+/- {stacking_scores.std()*2:.4f})")
        
        # é€‰æ‹©æœ€ä½³é›†æˆæ–¹æ³•
        if stacking_scores.mean() > voting_scores.mean():
            self.best_ensemble = stacking_clf
            self.best_ensemble_name = 'Stacking'
            self.best_ensemble_score = stacking_scores.mean()
        else:
            self.best_ensemble = voting_clf
            self.best_ensemble_name = 'Voting'
            self.best_ensemble_score = voting_scores.mean()
        
        print(f"\né€‰æ‹©æœ€ä½³é›†æˆ: {self.best_ensemble_name} (CV: {self.best_ensemble_score:.4f})")
        
        # æ¯”è¾ƒå•æ¨¡å‹å’Œé›†æˆæ¨¡å‹
        best_single_score = max(result['cv_mean'] for result in self.model_results)
        improvement = self.best_ensemble_score - best_single_score
        
        print(f"ç›¸æ¯”æœ€ä½³å•æ¨¡å‹æå‡: {improvement:.4f} ({improvement/best_single_score*100:.2f}%)")
        
        return self
    
    def train_final_model(self):
        """
        è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        """
        print("\nè®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        print("=" * 50)
        
        # é€‰æ‹©æœ€ç»ˆæ¨¡å‹
        best_single_score = max(result['cv_mean'] for result in self.model_results)
        
        if hasattr(self, 'best_ensemble') and self.best_ensemble_score > best_single_score:
            final_model = self.best_ensemble
            final_model_name = f'Ensemble ({self.best_ensemble_name})'
            final_score = self.best_ensemble_score
        else:
            # ä½¿ç”¨æœ€ä½³å•æ¨¡å‹
            best_model_name = self.model_results[0]['model_name']
            final_model = self.optimized_models[best_model_name]
            final_model_name = f'{best_model_name} (Optimized)'
            final_score = self.model_results[0]['cv_mean']
        
        print(f"æœ€ç»ˆæ¨¡å‹: {final_model_name}")
        print(f"é¢„æœŸCVåˆ†æ•°: {final_score:.4f}")
        
        # åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ
        print("åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ...")
        start_time = time.time()
        final_model.fit(self.X_train, self.y_train)
        training_time = time.time() - start_time
        
        print(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
        
        self.final_model = final_model
        self.final_model_name = final_model_name
        self.final_score = final_score
        
        return self
    
    def generate_predictions(self, output_file='submit.csv'):
        """
        ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹ç»“æœ
        """
        print("\nç”Ÿæˆé¢„æµ‹ç»“æœ...")
        print("=" * 50)
        
        # é¢„æµ‹
        print("é¢„æµ‹æµ‹è¯•é›†...")
        y_pred_encoded = self.final_model.predict(self.X_test)
        
        # è§£ç é¢„æµ‹ç»“æœ
        y_pred = self.target_encoder.inverse_transform(y_pred_encoded)
        
        # åˆ›å»ºæäº¤æ–‡ä»¶
        submission = pd.DataFrame({
            'ID': self.test_ids,
            'Tissue': y_pred
        })
        
        # ä¿å­˜
        submission.to_csv(output_file, index=False)
        print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # é¢„æµ‹åˆ†å¸ƒ
        pred_dist = pd.Series(y_pred).value_counts().sort_index()
        print(f"\né¢„æµ‹åˆ†å¸ƒ:")
        for tissue, count in pred_dist.items():
            print(f"  {tissue}: {count} ({count/len(y_pred)*100:.1f}%)")
        
        self.submission = submission
        return self
    
    def save_model(self, model_file='final_model.pkl'):
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œé¢„å¤„ç†å™¨
        """
        print(f"\nä¿å­˜æ¨¡å‹åˆ°: {model_file}")
        
        model_data = {
            'model': self.final_model,
            'model_name': self.final_model_name,
            'cv_score': self.final_score,
            'encoders': self.encoders,
            'scalers': self.scalers,
            'feature_selector': self.feature_selector,
            'feature_names': self.feature_names,
            'target_encoder': self.target_encoder,
            'training_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open(model_file, 'wb') as f:
            pickle.dump(model_data, f)
        
        print("æ¨¡å‹ä¿å­˜å®Œæˆ")
        
        return self
    
    def run_complete_pipeline(self, train_path='train.csv', test_path='test.csv', 
                            output_file='submit.csv', model_file='final_model.pkl'):
        """
        è¿è¡Œå®Œæ•´çš„é¢„æµ‹pipeline
        """
        print("=" * 80)
        print("           ç¯çŠ¶RNAç»„ç»‡è¡¨è¾¾é¢„æµ‹ - å®Œæ•´Pipeline")
        print("=" * 80)
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œå®Œæ•´æµç¨‹
            (self.load_data(train_path, test_path)
             .analyze_data()
             .feature_engineering()
             .create_models()
             .evaluate_models()
             .optimize_best_models()
             .create_ensemble()
             .train_final_model()
             .generate_predictions(output_file)
             .save_model(model_file))
            
            total_time = time.time() - start_time
            
            print("\n" + "=" * 80)
            print("                        Pipeline å®Œæˆ")
            print("=" * 80)
            print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
            print(f"æœ€ç»ˆæ¨¡å‹: {self.final_model_name}")
            print(f"é¢„æœŸæ€§èƒ½: {self.final_score:.4f} (Macro-F1)")
            print(f"é¢„æµ‹æ–‡ä»¶: {output_file}")
            print(f"æ¨¡å‹æ–‡ä»¶: {model_file}")
            print("=" * 80)
            
            return True
            
        except Exception as e:
            print(f"\nPipelineæ‰§è¡Œå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """
    ä¸»å‡½æ•°
    """
    # åˆ›å»ºpipelineå®ä¾‹
    pipeline = RNAPredictionPipeline(random_state=42)
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    success = pipeline.run_complete_pipeline(
        train_path='train.csv',
        test_path='test.csv',
        output_file='submit.csv',
        model_file='final_model.pkl'
    )
    
    if success:
        print("\nğŸ‰ é¢„æµ‹ä»»åŠ¡å®Œæˆï¼")
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   - submit.csv: æäº¤æ–‡ä»¶")
        print("   - final_model.pkl: è®­ç»ƒå¥½çš„æ¨¡å‹")
        print("\nğŸ’¡ æç¤º: å¯ä»¥å°è¯•è°ƒæ•´è¶…å‚æ•°æˆ–æ·»åŠ æ›´å¤šç‰¹å¾æ¥è¿›ä¸€æ­¥æå‡æ€§èƒ½")
    else:
        print("\nâŒ é¢„æµ‹ä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()