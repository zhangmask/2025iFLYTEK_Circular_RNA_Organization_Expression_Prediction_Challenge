#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¯çŠ¶RNAç»„ç»‡è¡¨è¾¾é¢„æµ‹ - æ”¹è¿›ç‰ˆæœ¬v3
åŸºäºåŸå§‹é«˜åˆ†æ¨¡å‹(0.36)çš„å¦ä¸€ç§ä¼˜åŒ–ç­–ç•¥

æ”¹è¿›ç­–ç•¥:
1. ç”Ÿç‰©å­¦æ„ä¹‰çš„ç‰¹å¾ç»„åˆ
2. ä¿å®ˆä½†ç¨³å®šçš„ç‰¹å¾é€‰æ‹©
3. è´å¶æ–¯ä¼˜åŒ–è¶…å‚æ•°æœç´¢
4. ä¼ªæ ‡ç­¾æŠ€æœ¯å’Œæ•°æ®å¢å¼º
5. ç²¾ç»†çš„ç±»åˆ«å¹³è¡¡å¤„ç†
6. ä¼˜åŒ–çš„æ¨¡å‹æƒé‡å’Œé›†æˆç­–ç•¥
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“
from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, classification_report
from sklearn.utils.class_weight import compute_class_weight
from sklearn.calibration import CalibratedClassifierCV

# XGBoostå’ŒLightGBM
import xgboost as xgb
import lightgbm as lgb

# CatBoost (å¯é€‰)
try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    print("CatBoostæœªå®‰è£…ï¼Œå°†è·³è¿‡CatBoostæ¨¡å‹")

# è´å¶æ–¯ä¼˜åŒ–
try:
    from skopt import BayesSearchCV
    from skopt.space import Real, Integer, Categorical
    BAYESIAN_OPT_AVAILABLE = True
except ImportError:
    from sklearn.model_selection import RandomizedSearchCV
    BAYESIAN_OPT_AVAILABLE = False
    print("scikit-optimizeæœªå®‰è£…ï¼Œä½¿ç”¨RandomizedSearchCV")

# å…¶ä»–å·¥å…·
import time
import pickle
from datetime import datetime
from collections import Counter
import math

class RNAPredictionPipelineV3:
    """
    ç¯çŠ¶RNAç»„ç»‡è¡¨è¾¾é¢„æµ‹Pipeline - æ”¹è¿›ç‰ˆæœ¬v3
    """
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        np.random.seed(random_state)
        
        # æ•°æ®å­˜å‚¨
        self.train_data = None
        self.test_data = None
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.test_ids = None
        
        # é¢„å¤„ç†å™¨
        self.encoders = {}
        self.scalers = {}
        self.feature_selector = None
        self.target_encoder = None
        
        # æ¨¡å‹ç›¸å…³
        self.models = {}
        self.optimized_models = {}
        self.model_results = []
        self.final_model = None
        
        print("RNAPredictionPipelineV3 åˆå§‹åŒ–å®Œæˆ")
        print(f"éšæœºç§å­: {random_state}")
        print(f"è´å¶æ–¯ä¼˜åŒ–å¯ç”¨: {BAYESIAN_OPT_AVAILABLE}")
        print(f"CatBoostå¯ç”¨: {CATBOOST_AVAILABLE}")
    
    def load_data(self, train_path='train.csv', test_path='test.csv'):
        """
        åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        """
        print("\nåŠ è½½æ•°æ®...")
        print("=" * 50)
        
        # åŠ è½½æ•°æ®
        self.train_data = pd.read_csv(train_path)
        self.test_data = pd.read_csv(test_path)
        
        print(f"è®­ç»ƒé›†å½¢çŠ¶: {self.train_data.shape}")
        print(f"æµ‹è¯•é›†å½¢çŠ¶: {self.test_data.shape}")
        
        # ä¿å­˜æµ‹è¯•é›†ID
        self.test_ids = self.test_data['ID'].values
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        print(f"\nè®­ç»ƒé›†ç¼ºå¤±å€¼: {self.train_data.isnull().sum().sum()}")
        print(f"æµ‹è¯•é›†ç¼ºå¤±å€¼: {self.test_data.isnull().sum().sum()}")
        
        # ç›®æ ‡å˜é‡åˆ†å¸ƒ
        target_dist = self.train_data['Tissue'].value_counts()
        print(f"\nç›®æ ‡å˜é‡åˆ†å¸ƒ:")
        for tissue, count in target_dist.items():
            print(f"  {tissue}: {count} ({count/len(self.train_data)*100:.1f}%)")
        
        return self
    
    def analyze_data(self):
        """
        æ•°æ®åˆ†æå’Œæ¢ç´¢
        """
        print("\næ•°æ®åˆ†æ...")
        print("=" * 50)
        
        # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        numeric_cols = self.train_data.select_dtypes(include=[np.number]).columns
        print(f"æ•°å€¼ç‰¹å¾æ•°é‡: {len(numeric_cols)}")
        
        categorical_cols = self.train_data.select_dtypes(include=['object']).columns
        categorical_cols = [col for col in categorical_cols if col not in ['ID', 'Tissue']]
        print(f"åˆ†ç±»ç‰¹å¾æ•°é‡: {len(categorical_cols)}")
        
        # ç‰¹å¾ç»Ÿè®¡
        print("\næ•°å€¼ç‰¹å¾ç»Ÿè®¡:")
        for col in numeric_cols[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            data = self.train_data[col]
            print(f"  {col}: å‡å€¼={data.mean():.3f}, æ ‡å‡†å·®={data.std():.3f}, èŒƒå›´=[{data.min():.3f}, {data.max():.3f}]")
        
        if len(categorical_cols) > 0:
            print("\nåˆ†ç±»ç‰¹å¾ç»Ÿè®¡:")
            for col in categorical_cols[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                unique_count = self.train_data[col].nunique()
                print(f"  {col}: {unique_count}ä¸ªå”¯ä¸€å€¼")
        
        return self
    
    def create_biological_features(self, data):
        """
        åˆ›å»ºç”Ÿç‰©å­¦æ„ä¹‰çš„ç‰¹å¾ç»„åˆ
        """
        print("åˆ›å»ºç”Ÿç‰©å­¦ç‰¹å¾...")
        
        # åŸºç¡€ç‰¹å¾
        gc_content = data.get('GC_Content', 0)
        length = data.get('Length', 1)
        energy = data.get('Energy', 0)
        mirna_binding = data.get('miRNA_Binding_Sites', 0)
        
        # ç”Ÿç‰©å­¦ç‰¹å¾ç»„åˆ
        features = {}
        
        # 1. GCå«é‡ç›¸å…³ç‰¹å¾
        features['GC_Length_Interaction'] = gc_content * np.log1p(length)
        features['GC_Energy_Ratio'] = gc_content / (abs(energy) + 1e-6)
        features['GC_Stability'] = gc_content * (1 / (abs(energy) + 1e-6))
        
        # 2. é•¿åº¦ç›¸å…³ç‰¹å¾
        features['Length_Log'] = np.log1p(length)
        features['Length_Sqrt'] = np.sqrt(length)
        features['Length_Energy_Density'] = abs(energy) / (length + 1e-6)
        
        # 3. èƒ½é‡ç›¸å…³ç‰¹å¾
        features['Energy_Abs'] = abs(energy)
        features['Energy_Normalized'] = energy / (length + 1e-6)
        features['Energy_Stability_Score'] = -energy / (gc_content * length + 1e-6)
        
        # 4. miRNAç»“åˆç›¸å…³ç‰¹å¾
        features['miRNA_Density'] = mirna_binding / (length + 1e-6)
        features['miRNA_GC_Interaction'] = mirna_binding * gc_content
        features['miRNA_Energy_Ratio'] = mirna_binding / (abs(energy) + 1e-6)
        
        # 5. å¤åˆç”Ÿç‰©å­¦ç‰¹å¾
        features['Structural_Complexity'] = (gc_content * length) / (abs(energy) + 1e-6)
        features['Binding_Efficiency'] = (mirna_binding * gc_content) / (length + 1e-6)
        features['Thermodynamic_Score'] = (gc_content * length) / (abs(energy) + mirna_binding + 1e-6)
        
        return features
    
    def feature_engineering(self):
        """
        ç‰¹å¾å·¥ç¨‹
        """
        print("\nç‰¹å¾å·¥ç¨‹...")
        print("=" * 50)
        
        # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®è¿›è¡Œä¸€è‡´çš„ç‰¹å¾å·¥ç¨‹
        train_features = self.train_data.drop(['ID', 'Tissue'], axis=1)
        test_features = self.test_data.drop(['ID'], axis=1)
        
        # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
        common_cols = list(set(train_features.columns) & set(test_features.columns))
        train_features = train_features[common_cols]
        test_features = test_features[common_cols]
        
        all_features = pd.concat([train_features, test_features], axis=0, ignore_index=True)
        
        print(f"åŸå§‹ç‰¹å¾æ•°é‡: {len(common_cols)}")
        
        # åˆ›å»ºç”Ÿç‰©å­¦ç‰¹å¾
        bio_features_list = []
        for idx in range(len(all_features)):
            row_data = all_features.iloc[idx].to_dict()
            bio_features = self.create_biological_features(row_data)
            bio_features_list.append(bio_features)
        
        bio_features_df = pd.DataFrame(bio_features_list)
        print(f"ç”Ÿç‰©å­¦ç‰¹å¾æ•°é‡: {len(bio_features_df.columns)}")
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        all_features_combined = pd.concat([all_features, bio_features_df], axis=1)
        
        # å¤„ç†ç¼ºå¤±å€¼
        numeric_cols = all_features_combined.select_dtypes(include=[np.number]).columns
        all_features_combined[numeric_cols] = all_features_combined[numeric_cols].fillna(
            all_features_combined[numeric_cols].median()
        )
        
        # å¤„ç†åˆ†ç±»ç‰¹å¾
        categorical_cols = all_features_combined.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if col not in self.encoders:
                self.encoders[col] = LabelEncoder()
                all_features_combined[col] = all_features_combined[col].fillna('Unknown')
                self.encoders[col].fit(all_features_combined[col])
            
            all_features_combined[col] = all_features_combined[col].fillna('Unknown')
            all_features_combined[col] = self.encoders[col].transform(all_features_combined[col])
        
        # åˆ†ç¦»è®­ç»ƒå’Œæµ‹è¯•é›†
        n_train = len(self.train_data)
        self.X_train = all_features_combined.iloc[:n_train].copy()
        self.X_test = all_features_combined.iloc[n_train:].copy()
        
        # å¤„ç†ç›®æ ‡å˜é‡
        self.target_encoder = LabelEncoder()
        self.y_train = self.target_encoder.fit_transform(self.train_data['Tissue'])
        
        print(f"æœ€ç»ˆç‰¹å¾æ•°é‡: {self.X_train.shape[1]}")
        print(f"è®­ç»ƒé›†å½¢çŠ¶: {self.X_train.shape}")
        print(f"æµ‹è¯•é›†å½¢çŠ¶: {self.X_test.shape}")
        
        # ä¿å­˜ç‰¹å¾åç§°
        self.feature_names = list(self.X_train.columns)
        
        return self
    
    def conservative_feature_selection(self):
        """
        ä¿å®ˆä½†ç¨³å®šçš„ç‰¹å¾é€‰æ‹©
        """
        print("\nä¿å®ˆç‰¹å¾é€‰æ‹©...")
        print("=" * 50)
        
        # 1. ç§»é™¤ä½æ–¹å·®ç‰¹å¾
        from sklearn.feature_selection import VarianceThreshold
        var_selector = VarianceThreshold(threshold=0.01)
        X_var = var_selector.fit_transform(self.X_train)
        selected_features = np.array(self.feature_names)[var_selector.get_support()]
        
        print(f"æ–¹å·®ç­›é€‰åç‰¹å¾æ•°é‡: {len(selected_features)}")
        
        # 2. ç›¸å…³æ€§ç­›é€‰
        X_var_df = pd.DataFrame(X_var, columns=selected_features)
        corr_matrix = X_var_df.corr().abs()
        
        # ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾
        upper_tri = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]
        X_corr = X_var_df.drop(columns=to_drop)
        
        print(f"ç›¸å…³æ€§ç­›é€‰åç‰¹å¾æ•°é‡: {X_corr.shape[1]}")
        
        # 3. ç»Ÿè®¡æ˜¾è‘—æ€§ç­›é€‰
        k_best = min(200, X_corr.shape[1])  # ä¿å®ˆé€‰æ‹©
        selector = SelectKBest(score_func=f_classif, k=k_best)
        X_selected = selector.fit_transform(X_corr, self.y_train)
        
        final_features = X_corr.columns[selector.get_support()]
        print(f"æœ€ç»ˆé€‰æ‹©ç‰¹å¾æ•°é‡: {len(final_features)}")
        
        # æ›´æ–°æ•°æ®
        self.X_train = pd.DataFrame(X_selected, columns=final_features)
        
        # å¯¹æµ‹è¯•é›†åº”ç”¨ç›¸åŒçš„å˜æ¢
        X_test_var = var_selector.transform(self.X_test)
        X_test_var_df = pd.DataFrame(X_test_var, columns=selected_features)
        X_test_corr = X_test_var_df.drop(columns=to_drop)
        X_test_selected = selector.transform(X_test_corr)
        self.X_test = pd.DataFrame(X_test_selected, columns=final_features)
        
        # ä¿å­˜é€‰æ‹©å™¨
        self.feature_selector = {
            'var_selector': var_selector,
            'corr_features': list(X_corr.columns),
            'kbest_selector': selector,
            'final_features': list(final_features)
        }
        
        return self
    
    def create_models(self):
        """
        åˆ›å»ºåŸºç¡€æ¨¡å‹
        """
        print("\nåˆ›å»ºæ¨¡å‹...")
        print("=" * 50)
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_weights = compute_class_weight(
            'balanced', 
            classes=np.unique(self.y_train), 
            y=self.y_train
        )
        class_weight_dict = dict(zip(np.unique(self.y_train), class_weights))
        
        print(f"ç±»åˆ«æƒé‡: {class_weight_dict}")
        
        # åŸºç¡€æ¨¡å‹é…ç½®
        self.models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                class_weight='balanced',
                random_state=self.random_state,
                n_jobs=-1
            ),
            
            'ExtraTrees': ExtraTreesClassifier(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                class_weight='balanced',
                random_state=self.random_state,
                n_jobs=-1
            ),
            
            'XGBoost': xgb.XGBClassifier(
                n_estimators=200,
                max_depth=8,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=self.random_state,
                n_jobs=-1,
                eval_metric='mlogloss'
            ),
            
            'LightGBM': lgb.LGBMClassifier(
                n_estimators=200,
                max_depth=8,
                learning_rate=0.1,
                num_leaves=50,
                subsample=0.8,
                colsample_bytree=0.8,
                class_weight='balanced',
                random_state=self.random_state,
                n_jobs=-1,
                verbose=-1
            )
        }
        
        if CATBOOST_AVAILABLE:
            self.models['CatBoost'] = cb.CatBoostClassifier(
                iterations=200,
                depth=8,
                learning_rate=0.1,
                class_weights=list(class_weights),
                random_seed=self.random_state,
                verbose=False
            )
        
        print(f"åˆ›å»ºäº† {len(self.models)} ä¸ªåŸºç¡€æ¨¡å‹")
        
        return self
    
    def evaluate_models(self):
        """
        è¯„ä¼°åŸºç¡€æ¨¡å‹æ€§èƒ½
        """
        print("\nè¯„ä¼°åŸºç¡€æ¨¡å‹...")
        print("=" * 50)
        
        cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        results = []
        
        for name, model in self.models.items():
            print(f"\nè¯„ä¼° {name}...")
            
            try:
                start_time = time.time()
                cv_scores = cross_val_score(
                    model, self.X_train, self.y_train,
                    cv=cv_strategy, scoring='f1_macro', n_jobs=-1
                )
                training_time = time.time() - start_time
                
                result = {
                    'model_name': name,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std(),
                    'cv_scores': cv_scores,
                    'training_time': training_time
                }
                
                results.append(result)
                
                print(f"  CV Macro-F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
                print(f"  è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
                
            except Exception as e:
                print(f"  é”™è¯¯: {str(e)}")
        
        # æ’åºç»“æœ
        results.sort(key=lambda x: x['cv_mean'], reverse=True)
        
        print("\næ¨¡å‹æ€§èƒ½æ’è¡Œ:")
        print("-" * 60)
        for i, result in enumerate(results, 1):
            print(f"{i}. {result['model_name']:15s} - CV: {result['cv_mean']:.4f} (+/- {result['cv_std']*2:.4f})")
        
        self.model_results = results
        return self
    
    def bayesian_optimization(self, top_k=3):
        """
        è´å¶æ–¯ä¼˜åŒ–è¶…å‚æ•°
        """
        print(f"\nè´å¶æ–¯ä¼˜åŒ– (Top {top_k} æ¨¡å‹)...")
        print("=" * 50)
        
        top_models = self.model_results[:top_k]
        optimized_models = {}
        
        # å®šä¹‰æœç´¢ç©ºé—´
        if BAYESIAN_OPT_AVAILABLE:
            search_spaces = {
                'RandomForest': {
                    'n_estimators': Integer(100, 500),
                    'max_depth': Integer(10, 30),
                    'min_samples_split': Integer(2, 20),
                    'min_samples_leaf': Integer(1, 10),
                    'max_features': Categorical(['sqrt', 'log2', 0.8])
                },
                'LightGBM': {
                    'n_estimators': Integer(100, 500),
                    'max_depth': Integer(6, 15),
                    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),
                    'num_leaves': Integer(20, 100),
                    'subsample': Real(0.6, 1.0)
                },
                'XGBoost': {
                    'n_estimators': Integer(100, 500),
                    'max_depth': Integer(6, 15),
                    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),
                    'subsample': Real(0.6, 1.0),
                    'colsample_bytree': Real(0.6, 1.0)
                }
            }
        else:
            search_spaces = {
                'RandomForest': {
                    'n_estimators': [100, 200, 300, 400, 500],
                    'max_depth': [10, 15, 20, 25, 30],
                    'min_samples_split': [2, 5, 10, 15, 20],
                    'min_samples_leaf': [1, 2, 5, 10],
                    'max_features': ['sqrt', 'log2', 0.8]
                },
                'LightGBM': {
                    'n_estimators': [100, 200, 300, 400, 500],
                    'max_depth': [6, 8, 10, 12, 15],
                    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],
                    'num_leaves': [20, 40, 60, 80, 100],
                    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0]
                },
                'XGBoost': {
                    'n_estimators': [100, 200, 300, 400, 500],
                    'max_depth': [6, 8, 10, 12, 15],
                    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],
                    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
                    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0]
                }
            }
        
        cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)
        
        for model_result in top_models:
            model_name = model_result['model_name']
            
            if model_name not in search_spaces:
                print(f"è·³è¿‡ {model_name} (æ— æœç´¢ç©ºé—´å®šä¹‰)")
                optimized_models[model_name] = self.models[model_name]
                continue
            
            print(f"\nä¼˜åŒ– {model_name}...")
            
            base_model = self.models[model_name]
            search_space = search_spaces[model_name]
            
            if BAYESIAN_OPT_AVAILABLE:
                search = BayesSearchCV(
                    estimator=base_model,
                    search_spaces=search_space,
                    n_iter=20,
                    cv=cv_strategy,
                    scoring='f1_macro',
                    n_jobs=-1,
                    random_state=self.random_state
                )
            else:
                # ä½¿ç”¨é¢„å®šä¹‰çš„æœç´¢ç©ºé—´ï¼ˆå·²ç»æ˜¯listæ ¼å¼ï¼‰
                param_dist = search_space
                
                search = RandomizedSearchCV(
                    estimator=base_model,
                    param_distributions=param_dist,
                    n_iter=20,
                    cv=cv_strategy,
                    scoring='f1_macro',
                    n_jobs=-1,
                    random_state=self.random_state
                )
            
            start_time = time.time()
            search.fit(self.X_train, self.y_train)
            search_time = time.time() - start_time
            
            optimized_models[model_name] = search.best_estimator_
            
            print(f"  æœ€ä½³CVåˆ†æ•°: {search.best_score_:.4f}")
            print(f"  ä¼˜åŒ–æ—¶é—´: {search_time:.2f}ç§’")
            print(f"  æœ€ä½³å‚æ•°: {search.best_params_}")
        
        self.optimized_models = optimized_models
        return self
    
    def create_ensemble_with_weights(self):
        """
        åˆ›å»ºåŠ æƒé›†æˆæ¨¡å‹
        """
        print("\nåˆ›å»ºåŠ æƒé›†æˆ...")
        print("=" * 50)
        
        # åŸºäºCVæ€§èƒ½è®¡ç®—æƒé‡
        model_weights = {}
        total_score = 0
        
        for result in self.model_results:
            if result['model_name'] in self.optimized_models:
                model_weights[result['model_name']] = result['cv_mean']
                total_score += result['cv_mean']
        
        # å½’ä¸€åŒ–æƒé‡
        for name in model_weights:
            model_weights[name] = model_weights[name] / total_score
        
        print(f"æ¨¡å‹æƒé‡: {model_weights}")
        
        # å‡†å¤‡åŸºæ¨¡å‹
        base_models = []
        for name, model in self.optimized_models.items():
            # ä½¿ç”¨æ ¡å‡†çš„åˆ†ç±»å™¨
            calibrated_model = CalibratedClassifierCV(model, cv=3, method='isotonic')
            base_models.append((name, calibrated_model))
        
        # åˆ›å»ºåŠ æƒæŠ•ç¥¨åˆ†ç±»å™¨
        weights = [model_weights.get(name, 1.0) for name, _ in base_models]
        
        self.ensemble_model = VotingClassifier(
            estimators=base_models,
            voting='soft',
            weights=weights,
            n_jobs=-1
        )
        
        # è¯„ä¼°é›†æˆæ¨¡å‹
        cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        ensemble_scores = cross_val_score(
            self.ensemble_model, self.X_train, self.y_train,
            cv=cv_strategy, scoring='f1_macro', n_jobs=-1
        )
        
        self.ensemble_score = ensemble_scores.mean()
        
        print(f"\nåŠ æƒé›†æˆCVåˆ†æ•°: {ensemble_scores.mean():.4f} (+/- {ensemble_scores.std()*2:.4f})")
        
        # æ¯”è¾ƒæœ€ä½³å•æ¨¡å‹
        best_single_score = max(result['cv_mean'] for result in self.model_results)
        improvement = self.ensemble_score - best_single_score
        
        print(f"ç›¸æ¯”æœ€ä½³å•æ¨¡å‹æå‡: {improvement:.4f} ({improvement/best_single_score*100:.2f}%)")
        
        return self
    
    def pseudo_labeling(self, confidence_threshold=0.9):
        """
        ä¼ªæ ‡ç­¾æŠ€æœ¯
        """
        print(f"\nä¼ªæ ‡ç­¾æŠ€æœ¯ (ç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold})...")
        print("=" * 50)
        
        # è®­ç»ƒåˆå§‹æ¨¡å‹
        best_model_name = self.model_results[0]['model_name']
        initial_model = self.optimized_models[best_model_name]
        initial_model.fit(self.X_train, self.y_train)
        
        # é¢„æµ‹æµ‹è¯•é›†æ¦‚ç‡
        test_probs = initial_model.predict_proba(self.X_test)
        max_probs = np.max(test_probs, axis=1)
        
        # é€‰æ‹©é«˜ç½®ä¿¡åº¦æ ·æœ¬
        high_conf_mask = max_probs >= confidence_threshold
        high_conf_indices = np.where(high_conf_mask)[0]
        
        if len(high_conf_indices) > 0:
            # è·å–ä¼ªæ ‡ç­¾
            pseudo_labels = initial_model.predict(self.X_test[high_conf_mask])
            
            # æ‰©å±•è®­ç»ƒé›†
            X_extended = pd.concat([
                self.X_train,
                self.X_test.iloc[high_conf_indices]
            ], axis=0, ignore_index=True)
            
            y_extended = np.concatenate([
                self.y_train,
                pseudo_labels
            ])
            
            print(f"æ·»åŠ äº† {len(high_conf_indices)} ä¸ªä¼ªæ ‡ç­¾æ ·æœ¬")
            print(f"æ‰©å±•åè®­ç»ƒé›†å¤§å°: {len(X_extended)}")
            
            # é‡æ–°è¯„ä¼°æ¨¡å‹
            cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)
            extended_scores = cross_val_score(
                initial_model, X_extended, y_extended,
                cv=cv_strategy, scoring='f1_macro', n_jobs=-1
            )
            
            print(f"ä¼ªæ ‡ç­¾åCVåˆ†æ•°: {extended_scores.mean():.4f} (+/- {extended_scores.std()*2:.4f})")
            
            # å¦‚æœæ€§èƒ½æå‡ï¼Œä½¿ç”¨æ‰©å±•æ•°æ®é›†
            original_score = self.model_results[0]['cv_mean']
            if extended_scores.mean() > original_score:
                self.X_train_extended = X_extended
                self.y_train_extended = y_extended
                self.use_pseudo_labels = True
                print("ä¼ªæ ‡ç­¾æå‡äº†æ€§èƒ½ï¼Œå°†ä½¿ç”¨æ‰©å±•æ•°æ®é›†")
            else:
                self.use_pseudo_labels = False
                print("ä¼ªæ ‡ç­¾æœªæå‡æ€§èƒ½ï¼Œä½¿ç”¨åŸå§‹æ•°æ®é›†")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°é«˜ç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾æ ·æœ¬")
            self.use_pseudo_labels = False
        
        return self
    
    def train_final_model(self):
        """
        è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        """
        print("\nè®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        print("=" * 50)
        
        # é€‰æ‹©è®­ç»ƒæ•°æ®
        if hasattr(self, 'use_pseudo_labels') and self.use_pseudo_labels:
            X_final = self.X_train_extended
            y_final = self.y_train_extended
            print("ä½¿ç”¨ä¼ªæ ‡ç­¾æ‰©å±•çš„è®­ç»ƒé›†")
        else:
            X_final = self.X_train
            y_final = self.y_train
            print("ä½¿ç”¨åŸå§‹è®­ç»ƒé›†")
        
        # é€‰æ‹©æœ€ç»ˆæ¨¡å‹
        if hasattr(self, 'ensemble_model') and self.ensemble_score > self.model_results[0]['cv_mean']:
            final_model = self.ensemble_model
            final_model_name = 'Weighted Ensemble'
            final_score = self.ensemble_score
        else:
            best_model_name = self.model_results[0]['model_name']
            final_model = self.optimized_models[best_model_name]
            final_model_name = f'{best_model_name} (Optimized)'
            final_score = self.model_results[0]['cv_mean']
        
        print(f"æœ€ç»ˆæ¨¡å‹: {final_model_name}")
        print(f"é¢„æœŸCVåˆ†æ•°: {final_score:.4f}")
        
        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        print("åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ...")
        start_time = time.time()
        final_model.fit(X_final, y_final)
        training_time = time.time() - start_time
        
        print(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
        
        self.final_model = final_model
        self.final_model_name = final_model_name
        self.final_score = final_score
        
        return self
    
    def generate_predictions(self, output_file='submit_v3.csv'):
        """
        ç”Ÿæˆé¢„æµ‹ç»“æœ
        """
        print("\nç”Ÿæˆé¢„æµ‹ç»“æœ...")
        print("=" * 50)
        
        # é¢„æµ‹
        print("é¢„æµ‹æµ‹è¯•é›†...")
        y_pred_encoded = self.final_model.predict(self.X_test)
        
        # è§£ç é¢„æµ‹ç»“æœ
        y_pred = self.target_encoder.inverse_transform(y_pred_encoded)
        
        # åˆ›å»ºæäº¤æ–‡ä»¶
        submission = pd.DataFrame({
            'ID': self.test_ids,
            'Tissue': y_pred
        })
        
        # ä¿å­˜
        submission.to_csv(output_file, index=False)
        print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # é¢„æµ‹åˆ†å¸ƒ
        pred_dist = pd.Series(y_pred).value_counts().sort_index()
        print(f"\né¢„æµ‹åˆ†å¸ƒ:")
        for tissue, count in pred_dist.items():
            print(f"  {tissue}: {count} ({count/len(y_pred)*100:.1f}%)")
        
        self.submission = submission
        return self
    
    def run_complete_pipeline(self, train_path='train.csv', test_path='test.csv', 
                            output_file='submit_v3.csv'):
        """
        è¿è¡Œå®Œæ•´çš„é¢„æµ‹pipeline
        """
        print("=" * 80)
        print("           ç¯çŠ¶RNAç»„ç»‡è¡¨è¾¾é¢„æµ‹ - Pipeline V3")
        print("=" * 80)
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œå®Œæ•´æµç¨‹
            (self.load_data(train_path, test_path)
             .analyze_data()
             .feature_engineering()
             .conservative_feature_selection()
             .create_models()
             .evaluate_models()
             .bayesian_optimization()
             .create_ensemble_with_weights()
             .pseudo_labeling()
             .train_final_model()
             .generate_predictions(output_file))
            
            total_time = time.time() - start_time
            
            print("\n" + "=" * 80)
            print("                        Pipeline V3 å®Œæˆ")
            print("=" * 80)
            print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
            print(f"æœ€ç»ˆæ¨¡å‹: {self.final_model_name}")
            print(f"é¢„æœŸæ€§èƒ½: {self.final_score:.4f} (Macro-F1)")
            print(f"é¢„æµ‹æ–‡ä»¶: {output_file}")
            print("=" * 80)
            
            return True
            
        except Exception as e:
            print(f"\nPipelineæ‰§è¡Œå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """
    ä¸»å‡½æ•°
    """
    # åˆ›å»ºpipelineå®ä¾‹
    pipeline = RNAPredictionPipelineV3(random_state=42)
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    success = pipeline.run_complete_pipeline(
        train_path='train.csv',
        test_path='test.csv',
        output_file='submit_v3.csv'
    )
    
    if success:
        print("\nğŸ‰ é¢„æµ‹ä»»åŠ¡å®Œæˆï¼")
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   - submit_v3.csv: æäº¤æ–‡ä»¶")
        print("\nğŸ’¡ æ”¹è¿›ç­–ç•¥:")
        print("   - ç”Ÿç‰©å­¦ç‰¹å¾ç»„åˆ")
        print("   - ä¿å®ˆç‰¹å¾é€‰æ‹©")
        print("   - è´å¶æ–¯ä¼˜åŒ–")
        print("   - ä¼ªæ ‡ç­¾æŠ€æœ¯")
        print("   - åŠ æƒé›†æˆ")
    else:
        print("\nâŒ é¢„æµ‹ä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()