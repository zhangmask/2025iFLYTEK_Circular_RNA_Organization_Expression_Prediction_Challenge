{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环状RNA组织表达预测 - 模型评估与性能分析\n",
    "\n",
    "本notebook用于详细评估训练好的模型，包括：\n",
    "1. 模型性能评估\n",
    "2. 混淆矩阵分析\n",
    "3. 分类报告\n",
    "4. 特征重要性分析\n",
    "5. 错误分析\n",
    "6. 模型解释性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# 机器学习评估库\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    f1_score, accuracy_score, precision_score, recall_score,\n",
    "    precision_recall_fscore_support, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "\n",
    "# 模型解释库\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"SHAP not available, will skip SHAP analysis\")\n",
    "    SHAP_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    PERMUTATION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PERMUTATION_AVAILABLE = False\n",
    "\n",
    "# 设置显示选项\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置图形样式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加载模型和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练好的模型\n",
    "with open('final_model.pkl', 'rb') as f:\n",
    "    model_data = pickle.load(f)\n",
    "\n",
    "final_model = model_data['model']\n",
    "model_name = model_data['model_name']\n",
    "validation_score = model_data['validation_score']\n",
    "feature_names = model_data['feature_names']\n",
    "target_encoder = model_data['target_encoder']\n",
    "\n",
    "print(f\"加载的模型: {model_name}\")\n",
    "print(f\"验证集Macro-F1: {validation_score:.4f}\")\n",
    "print(f\"特征数量: {len(feature_names)}\")\n",
    "print(f\"目标类别: {target_encoder.classes_}\")\n",
    "\n",
    "# 加载处理后的数据\n",
    "with open('processed_data.pkl', 'rb') as f:\n",
    "    processed_data = pickle.load(f)\n",
    "\n",
    "X_train = processed_data['X_train']\n",
    "X_test = processed_data['X_test']\n",
    "y_train = processed_data['y_train']\n",
    "X_train_split = processed_data['X_train_split']\n",
    "X_val_split = processed_data['X_val_split']\n",
    "y_train_split = processed_data['y_train_split']\n",
    "y_val_split = processed_data['y_val_split']\n",
    "test_ids = processed_data['test_ids']\n",
    "\n",
    "print(f\"\\n数据形状:\")\n",
    "print(f\"训练集: {X_train.shape}\")\n",
    "print(f\"测试集: {X_test.shape}\")\n",
    "print(f\"训练分割: {X_train_split.shape}\")\n",
    "print(f\"验证分割: {X_val_split.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 生成预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成预测结果\n",
    "print(\"生成预测结果...\")\n",
    "\n",
    "# 训练集预测\n",
    "y_train_pred = final_model.predict(X_train_split)\n",
    "y_train_proba = final_model.predict_proba(X_train_split)\n",
    "\n",
    "# 验证集预测\n",
    "y_val_pred = final_model.predict(X_val_split)\n",
    "y_val_proba = final_model.predict_proba(X_val_split)\n",
    "\n",
    "# 完整训练集预测（用于最终评估）\n",
    "y_full_train_pred = final_model.predict(X_train)\n",
    "y_full_train_proba = final_model.predict_proba(X_train)\n",
    "\n",
    "print(\"预测结果生成完成\")\n",
    "print(f\"训练集预测形状: {y_train_pred.shape}\")\n",
    "print(f\"验证集预测形状: {y_val_pred.shape}\")\n",
    "print(f\"完整训练集预测形状: {y_full_train_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 详细性能评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算各种评估指标\n",
    "def calculate_metrics(y_true, y_pred, y_proba=None, dataset_name=''):\n",
    "    \"\"\"计算详细的评估指标\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # 基本指标\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro')\n",
    "    metrics['f1_micro'] = f1_score(y_true, y_pred, average='micro')\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted')\n",
    "    metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro')\n",
    "    metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    # 每个类别的详细指标\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
    "    \n",
    "    for i, class_name in enumerate(target_encoder.classes_):\n",
    "        metrics[f'precision_{class_name}'] = precision[i]\n",
    "        metrics[f'recall_{class_name}'] = recall[i]\n",
    "        metrics[f'f1_{class_name}'] = f1[i]\n",
    "        metrics[f'support_{class_name}'] = support[i]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# 计算训练集和验证集指标\n",
    "train_metrics = calculate_metrics(y_train_split, y_train_pred, y_train_proba, 'Train')\n",
    "val_metrics = calculate_metrics(y_val_split, y_val_pred, y_val_proba, 'Validation')\n",
    "full_train_metrics = calculate_metrics(y_train, y_full_train_pred, y_full_train_proba, 'Full Train')\n",
    "\n",
    "# 创建指标比较表\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Train Split': [\n",
    "        train_metrics['accuracy'],\n",
    "        train_metrics['f1_macro'],\n",
    "        train_metrics['f1_micro'],\n",
    "        train_metrics['f1_weighted'],\n",
    "        train_metrics['precision_macro'],\n",
    "        train_metrics['recall_macro']\n",
    "    ],\n",
    "    'Validation Split': [\n",
    "        val_metrics['accuracy'],\n",
    "        val_metrics['f1_macro'],\n",
    "        val_metrics['f1_micro'],\n",
    "        val_metrics['f1_weighted'],\n",
    "        val_metrics['precision_macro'],\n",
    "        val_metrics['recall_macro']\n",
    "    ],\n",
    "    'Full Train': [\n",
    "        full_train_metrics['accuracy'],\n",
    "        full_train_metrics['f1_macro'],\n",
    "        full_train_metrics['f1_micro'],\n",
    "        full_train_metrics['f1_weighted'],\n",
    "        full_train_metrics['precision_macro'],\n",
    "        full_train_metrics['recall_macro']\n",
    "    ]\n",
    "}, index=['Accuracy', 'F1-Macro', 'F1-Micro', 'F1-Weighted', 'Precision-Macro', 'Recall-Macro'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"                    模型性能评估\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"整体性能指标:\")\n",
    "display(metrics_comparison.round(4))\n",
    "\n",
    "# 过拟合分析\n",
    "overfitting_analysis = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1-Macro', 'Precision-Macro', 'Recall-Macro'],\n",
    "    'Train': [train_metrics['accuracy'], train_metrics['f1_macro'], \n",
    "             train_metrics['precision_macro'], train_metrics['recall_macro']],\n",
    "    'Validation': [val_metrics['accuracy'], val_metrics['f1_macro'], \n",
    "                  val_metrics['precision_macro'], val_metrics['recall_macro']],\n",
    "    'Difference': [\n",
    "        train_metrics['accuracy'] - val_metrics['accuracy'],\n",
    "        train_metrics['f1_macro'] - val_metrics['f1_macro'],\n",
    "        train_metrics['precision_macro'] - val_metrics['precision_macro'],\n",
    "        train_metrics['recall_macro'] - val_metrics['recall_macro']\n",
    "    ]\n",
    "})\n",
    "\n",
    "overfitting_analysis['Overfitting_Risk'] = overfitting_analysis['Difference'].apply(\n",
    "    lambda x: 'High' if x > 0.1 else 'Medium' if x > 0.05 else 'Low'\n",
    ")\n",
    "\n",
    "print(\"\\n过拟合分析:\")\n",
    "display(overfitting_analysis.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 混淆矩阵分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成混淆矩阵\n",
    "def plot_confusion_matrix(y_true, y_pred, title, class_names):\n",
    "    \"\"\"绘制混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # 计算百分比\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # 绘制混淆矩阵\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    plt.title(f'{title} - 混淆矩阵 (计数)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('预测类别', fontsize=12)\n",
    "    plt.ylabel('真实类别', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 绘制百分比混淆矩阵\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Percentage (%)'})\n",
    "    \n",
    "    plt.title(f'{title} - 混淆矩阵 (百分比)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('预测类别', fontsize=12)\n",
    "    plt.ylabel('真实类别', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cm, cm_percent\n",
    "\n",
    "# 绘制验证集混淆矩阵\n",
    "print(\"验证集混淆矩阵分析:\")\n",
    "val_cm, val_cm_percent = plot_confusion_matrix(\n",
    "    y_val_split, y_val_pred, '验证集', target_encoder.classes_\n",
    ")\n",
    "\n",
    "# 分析混淆矩阵\n",
    "print(\"\\n混淆矩阵分析:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 计算每个类别的准确率\n",
    "class_accuracy = np.diag(val_cm) / np.sum(val_cm, axis=1)\n",
    "\n",
    "for i, class_name in enumerate(target_encoder.classes_):\n",
    "    print(f\"{class_name}: 准确率 {class_accuracy[i]:.3f} ({np.diag(val_cm)[i]}/{np.sum(val_cm, axis=1)[i]})\")\n",
    "\n",
    "# 找出最容易混淆的类别对\n",
    "print(\"\\n最容易混淆的类别对:\")\n",
    "confusion_pairs = []\n",
    "for i in range(len(target_encoder.classes_)):\n",
    "    for j in range(len(target_encoder.classes_)):\n",
    "        if i != j and val_cm[i, j] > 0:\n",
    "            confusion_pairs.append((\n",
    "                target_encoder.classes_[i], \n",
    "                target_encoder.classes_[j], \n",
    "                val_cm[i, j],\n",
    "                val_cm_percent[i, j]\n",
    "            ))\n",
    "\n",
    "# 按混淆数量排序\n",
    "confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for true_class, pred_class, count, percent in confusion_pairs[:10]:\n",
    "    print(f\"{true_class} → {pred_class}: {count} 次 ({percent:.1f}%)\")\n",
    "\n",
    "# 计算对角线占比（正确分类比例）\n",
    "diagonal_sum = np.trace(val_cm)\n",
    "total_sum = np.sum(val_cm)\n",
    "overall_accuracy = diagonal_sum / total_sum\n",
    "\n",
    "print(f\"\\n整体准确率: {overall_accuracy:.4f} ({diagonal_sum}/{total_sum})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 分类报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成详细的分类报告\n",
    "print(\"验证集分类报告:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# sklearn的分类报告\n",
    "val_report = classification_report(\n",
    "    y_val_split, y_val_pred, \n",
    "    target_names=target_encoder.classes_,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# 转换为DataFrame便于显示\n",
    "report_df = pd.DataFrame(val_report).transpose()\n",
    "\n",
    "# 重新排序列\n",
    "if 'support' in report_df.columns:\n",
    "    report_df = report_df[['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "display(report_df.round(4))\n",
    "\n",
    "# 打印文本格式的报告\n",
    "print(\"\\n文本格式分类报告:\")\n",
    "print(classification_report(\n",
    "    y_val_split, y_val_pred, \n",
    "    target_names=target_encoder.classes_\n",
    "))\n",
    "\n",
    "# 创建每个类别的性能可视化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 提取每个类别的指标\n",
    "classes = target_encoder.classes_\n",
    "precision_scores = [val_report[cls]['precision'] for cls in classes]\n",
    "recall_scores = [val_report[cls]['recall'] for cls in classes]\n",
    "f1_scores = [val_report[cls]['f1-score'] for cls in classes]\n",
    "support_counts = [val_report[cls]['support'] for cls in classes]\n",
    "\n",
    "x_pos = np.arange(len(classes))\n",
    "\n",
    "# 1. Precision\n",
    "axes[0, 0].bar(x_pos, precision_scores, alpha=0.8, color='skyblue')\n",
    "axes[0, 0].set_title('各类别Precision', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('类别')\n",
    "axes[0, 0].set_ylabel('Precision')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(classes, rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Recall\n",
    "axes[0, 1].bar(x_pos, recall_scores, alpha=0.8, color='lightcoral')\n",
    "axes[0, 1].set_title('各类别Recall', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('类别')\n",
    "axes[0, 1].set_ylabel('Recall')\n",
    "axes[0, 1].set_xticks(x_pos)\n",
    "axes[0, 1].set_xticklabels(classes, rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. F1-Score\n",
    "axes[1, 0].bar(x_pos, f1_scores, alpha=0.8, color='lightgreen')\n",
    "axes[1, 0].set_title('各类别F1-Score', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('类别')\n",
    "axes[1, 0].set_ylabel('F1-Score')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(classes, rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Support (样本数量)\n",
    "axes[1, 1].bar(x_pos, support_counts, alpha=0.8, color='gold')\n",
    "axes[1, 1].set_title('各类别样本数量', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('类别')\n",
    "axes[1, 1].set_ylabel('样本数量')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(classes, rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 特征重要性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征重要性分析\n",
    "print(\"特征重要性分析:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 获取特征重要性\n",
    "feature_importance = None\n",
    "\n",
    "# 尝试获取模型的特征重要性\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    # 树模型的特征重要性\n",
    "    feature_importance = final_model.feature_importances_\n",
    "    importance_type = 'Built-in Feature Importance'\n",
    "elif hasattr(final_model, 'coef_'):\n",
    "    # 线性模型的系数\n",
    "    feature_importance = np.abs(final_model.coef_).mean(axis=0)\n",
    "    importance_type = 'Coefficient Magnitude'\n",
    "elif hasattr(final_model, 'estimators_'):\n",
    "    # 集成模型\n",
    "    try:\n",
    "        # 尝试获取第一个估计器的特征重要性\n",
    "        if hasattr(final_model.estimators_[0], 'feature_importances_'):\n",
    "            feature_importance = np.mean([\n",
    "                est.feature_importances_ for est in final_model.estimators_\n",
    "            ], axis=0)\n",
    "            importance_type = 'Ensemble Feature Importance'\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# 如果模型没有内置特征重要性，使用排列重要性\n",
    "if feature_importance is None and PERMUTATION_AVAILABLE:\n",
    "    print(\"使用排列重要性计算特征重要性...\")\n",
    "    perm_importance = permutation_importance(\n",
    "        final_model, X_val_split, y_val_split, \n",
    "        n_repeats=5, random_state=42, scoring='f1_macro'\n",
    "    )\n",
    "    feature_importance = perm_importance.importances_mean\n",
    "    importance_type = 'Permutation Importance'\n",
    "\n",
    "if feature_importance is not None:\n",
    "    # 创建特征重要性DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"特征重要性类型: {importance_type}\")\n",
    "    print(f\"\\nTop 20 重要特征:\")\n",
    "    display(importance_df.head(20).round(6))\n",
    "    \n",
    "    # 可视化特征重要性\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Top 20特征\n",
    "    top_features = importance_df.head(20)\n",
    "    \n",
    "    plt.barh(range(len(top_features)), top_features['Importance'], alpha=0.8)\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('重要性分数')\n",
    "    plt.title(f'Top 20 特征重要性 ({importance_type})', fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 累积重要性分析\n",
    "    cumulative_importance = np.cumsum(importance_df['Importance'].values)\n",
    "    cumulative_importance = cumulative_importance / cumulative_importance[-1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'b-', linewidth=2)\n",
    "    plt.axhline(y=0.8, color='r', linestyle='--', alpha=0.7, label='80%重要性')\n",
    "    plt.axhline(y=0.9, color='orange', linestyle='--', alpha=0.7, label='90%重要性')\n",
    "    plt.axhline(y=0.95, color='green', linestyle='--', alpha=0.7, label='95%重要性')\n",
    "    plt.xlabel('特征数量')\n",
    "    plt.ylabel('累积重要性')\n",
    "    plt.title('特征重要性累积分布', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 分析需要多少特征达到不同的重要性阈值\n",
    "    thresholds = [0.8, 0.9, 0.95, 0.99]\n",
    "    print(\"\\n特征选择建议:\")\n",
    "    for threshold in thresholds:\n",
    "        n_features = np.argmax(cumulative_importance >= threshold) + 1\n",
    "        print(f\"达到{threshold*100:.0f}%重要性需要: {n_features} 个特征\")\n",
    "    \n",
    "    # 保存特征重要性\n",
    "    importance_df.to_csv('feature_importance.csv', index=False)\n",
    "    print(\"\\n特征重要性已保存到 feature_importance.csv\")\n",
    "    \n",
    "else:\n",
    "    print(\"无法获取特征重要性信息\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 错误分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 错误分析\n",
    "print(\"错误分析:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 找出预测错误的样本\n",
    "wrong_predictions = y_val_split != y_val_pred\n",
    "correct_predictions = y_val_split == y_val_pred\n",
    "\n",
    "print(f\"总样本数: {len(y_val_split)}\")\n",
    "print(f\"正确预测: {np.sum(correct_predictions)} ({np.mean(correct_predictions)*100:.2f}%)\")\n",
    "print(f\"错误预测: {np.sum(wrong_predictions)} ({np.mean(wrong_predictions)*100:.2f}%)\")\n",
    "\n",
    "# 分析每个类别的错误情况\n",
    "print(\"\\n各类别错误分析:\")\n",
    "error_analysis = []\n",
    "\n",
    "for i, class_name in enumerate(target_encoder.classes_):\n",
    "    class_mask = y_val_split == i\n",
    "    class_total = np.sum(class_mask)\n",
    "    class_correct = np.sum((y_val_split == i) & (y_val_pred == i))\n",
    "    class_wrong = class_total - class_correct\n",
    "    \n",
    "    if class_total > 0:\n",
    "        class_accuracy = class_correct / class_total\n",
    "        error_rate = class_wrong / class_total\n",
    "        \n",
    "        error_analysis.append({\n",
    "            'Class': class_name,\n",
    "            'Total': class_total,\n",
    "            'Correct': class_correct,\n",
    "            'Wrong': class_wrong,\n",
    "            'Accuracy': class_accuracy,\n",
    "            'Error_Rate': error_rate\n",
    "        })\n",
    "        \n",
    "        print(f\"{class_name}: {class_correct}/{class_total} 正确 (准确率: {class_accuracy:.3f}, 错误率: {error_rate:.3f})\")\n",
    "\n",
    "error_df = pd.DataFrame(error_analysis)\n",
    "\n",
    "# 可视化错误分析\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. 各类别准确率\n",
    "axes[0].bar(error_df['Class'], error_df['Accuracy'], alpha=0.8, color='lightblue')\n",
    "axes[0].set_title('各类别准确率', fontweight='bold')\n",
    "axes[0].set_xlabel('类别')\n",
    "axes[0].set_ylabel('准确率')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. 各类别错误数量\n",
    "axes[1].bar(error_df['Class'], error_df['Wrong'], alpha=0.8, color='lightcoral')\n",
    "axes[1].set_title('各类别错误数量', fontweight='bold')\n",
    "axes[1].set_xlabel('类别')\n",
    "axes[1].set_ylabel('错误数量')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 预测置信度分析\n",
    "print(\"\\n预测置信度分析:\")\n",
    "\n",
    "# 计算预测置信度（最大概率）\n",
    "prediction_confidence = np.max(y_val_proba, axis=1)\n",
    "\n",
    "# 分析正确和错误预测的置信度分布\n",
    "correct_confidence = prediction_confidence[correct_predictions]\n",
    "wrong_confidence = prediction_confidence[wrong_predictions]\n",
    "\n",
    "print(f\"正确预测平均置信度: {np.mean(correct_confidence):.4f}\")\n",
    "print(f\"错误预测平均置信度: {np.mean(wrong_confidence):.4f}\")\n",
    "print(f\"置信度差异: {np.mean(correct_confidence) - np.mean(wrong_confidence):.4f}\")\n",
    "\n",
    "# 可视化置信度分布\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(correct_confidence, bins=30, alpha=0.7, label='正确预测', color='green')\n",
    "plt.hist(wrong_confidence, bins=30, alpha=0.7, label='错误预测', color='red')\n",
    "plt.xlabel('预测置信度')\n",
    "plt.ylabel('频次')\n",
    "plt.title('预测置信度分布', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([correct_confidence, wrong_confidence], labels=['正确预测', '错误预测'])\n",
    "plt.ylabel('预测置信度')\n",
    "plt.title('预测置信度箱线图', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 低置信度预测分析\n",
    "low_confidence_threshold = 0.5\n",
    "low_confidence_mask = prediction_confidence < low_confidence_threshold\n",
    "\n",
    "print(f\"\\n低置信度预测分析 (置信度 < {low_confidence_threshold}):\")\n",
    "print(f\"低置信度预测数量: {np.sum(low_confidence_mask)}\")\n",
    "\n",
    "if np.sum(low_confidence_mask) > 0:\n",
    "    low_conf_accuracy = np.mean(correct_predictions[low_confidence_mask])\n",
    "    print(f\"低置信度预测准确率: {low_conf_accuracy:.4f}\")\n",
    "    \n",
    "    # 分析低置信度预测的类别分布\n",
    "    low_conf_true_classes = y_val_split[low_confidence_mask]\n",
    "    low_conf_pred_classes = y_val_pred[low_confidence_mask]\n",
    "    \n",
    "    print(\"\\n低置信度预测的真实类别分布:\")\n",
    "    for class_idx, class_name in enumerate(target_encoder.classes_):\n",
    "        count = np.sum(low_conf_true_classes == class_idx)\n",
    "        if count > 0:\n",
    "            print(f\"  {class_name}: {count} 个样本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 学习曲线分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习曲线分析\n",
    "print(\"生成学习曲线...\")\n",
    "\n",
    "# 创建一个简化的模型用于学习曲线分析（避免计算时间过长）\n",
    "if 'Random Forest' in model_name or 'XGBoost' in model_name or 'LightGBM' in model_name:\n",
    "    # 对于复杂模型，使用较少的估计器\n",
    "    if 'Random Forest' in model_name:\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        simple_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "    elif 'XGBoost' in model_name:\n",
    "        import xgboost as xgb\n",
    "        simple_model = xgb.XGBClassifier(n_estimators=50, random_state=42, eval_metric='mlogloss', verbosity=0)\n",
    "    elif 'LightGBM' in model_name:\n",
    "        import lightgbm as lgb\n",
    "        simple_model = lgb.LGBMClassifier(n_estimators=50, random_state=42, verbosity=-1)\n",
    "    else:\n",
    "        simple_model = final_model\n",
    "else:\n",
    "    simple_model = final_model\n",
    "\n",
    "try:\n",
    "    # 生成学习曲线\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        simple_model, X_train, y_train,\n",
    "        cv=3,  # 减少CV折数以加快计算\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 计算均值和标准差\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    # 绘制学习曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='训练集分数')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    \n",
    "    plt.plot(train_sizes, val_mean, 'o-', color='red', label='验证集分数')\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "    \n",
    "    plt.xlabel('训练样本数量')\n",
    "    plt.ylabel('Macro-F1 分数')\n",
    "    plt.title(f'{model_name} - 学习曲线', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 分析学习曲线\n",
    "    print(\"学习曲线分析:\")\n",
    "    print(f\"最终训练集分数: {train_mean[-1]:.4f} (+/- {train_std[-1]:.4f})\")\n",
    "    print(f\"最终验证集分数: {val_mean[-1]:.4f} (+/- {val_std[-1]:.4f})\")\n",
    "    \n",
    "    # 判断是否需要更多数据\n",
    "    if val_mean[-1] < val_mean[-2]:\n",
    "        print(\"建议: 验证集性能可能已经饱和，增加更多数据可能不会显著提升性能\")\n",
    "    else:\n",
    "        print(\"建议: 增加更多训练数据可能会进一步提升模型性能\")\n",
    "    \n",
    "    # 过拟合分析\n",
    "    overfitting_gap = train_mean[-1] - val_mean[-1]\n",
    "    if overfitting_gap > 0.1:\n",
    "        print(f\"警告: 存在明显过拟合 (差距: {overfitting_gap:.4f})\")\n",
    "    elif overfitting_gap > 0.05:\n",
    "        print(f\"注意: 存在轻微过拟合 (差距: {overfitting_gap:.4f})\")\n",
    "    else:\n",
    "        print(f\"良好: 过拟合程度较低 (差距: {overfitting_gap:.4f})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"学习曲线生成失败: {str(e)}\")\n",
    "    print(\"跳过学习曲线分析\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 模型解释性分析 (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP分析（如果可用）\n",
    "if SHAP_AVAILABLE:\n",
    "    print(\"SHAP模型解释性分析:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # 选择一个较小的样本子集进行SHAP分析\n",
    "        sample_size = min(100, len(X_val_split))\n",
    "        sample_indices = np.random.choice(len(X_val_split), sample_size, replace=False)\n",
    "        X_sample = X_val_split.iloc[sample_indices]\n",
    "        \n",
    "        print(f\"使用 {sample_size} 个样本进行SHAP分析...\")\n",
    "        \n",
    "        # 创建SHAP解释器\n",
    "        if hasattr(final_model, 'predict_proba'):\n",
    "            # 对于分类模型\n",
    "            if 'XGBoost' in model_name or 'LightGBM' in model_name:\n",
    "                explainer = shap.TreeExplainer(final_model)\n",
    "            else:\n",
    "                # 使用KernelExplainer作为通用解释器\n",
    "                background = shap.sample(X_train, 50)  # 使用50个样本作为背景\n",
    "                explainer = shap.KernelExplainer(final_model.predict_proba, background)\n",
    "        else:\n",
    "            background = shap.sample(X_train, 50)\n",
    "            explainer = shap.KernelExplainer(final_model.predict, background)\n",
    "        \n",
    "        # 计算SHAP值\n",
    "        print(\"计算SHAP值...\")\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "        \n",
    "        # 如果是多分类，shap_values是一个列表\n",
    "        if isinstance(shap_values, list):\n",
    "            # 绘制每个类别的SHAP总结图\n",
    "            for i, class_name in enumerate(