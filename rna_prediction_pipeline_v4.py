#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¯çŠ¶RNAç»„ç»‡è¡¨è¾¾é¢„æµ‹ Pipeline v4
åŸºäºåŸå§‹é«˜åˆ†æ¨¡å‹(0.36)çš„ä¼˜åŒ–ç‰ˆæœ¬

ä¸»è¦æ”¹è¿›:
1. ä¿ç•™åŸå§‹æ¨¡å‹çš„æˆåŠŸæ¶æ„
2. ä¼˜åŒ–ç‰¹å¾é€‰æ‹©ç­–ç•¥å’Œé˜ˆå€¼
3. ç²¾ç»†åŒ–è¶…å‚æ•°ç½‘æ ¼
4. æ”¹è¿›é›†æˆæƒé‡ç­–ç•¥
5. å¢å¼ºæ•°æ®é¢„å¤„ç†
"""

import pandas as pd
import numpy as np
import warnings
from datetime import datetime
import time
import pickle
from collections import Counter

# æœºå™¨å­¦ä¹ åº“
from sklearn.model_selection import (
    StratifiedKFold, cross_val_score, RandomizedSearchCV
)
from sklearn.preprocessing import LabelEncoder, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.ensemble import (
    RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, StackingClassifier
)
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# XGBoostå’ŒLightGBM
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("è­¦å‘Š: XGBoostæœªå®‰è£…")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("è­¦å‘Š: LightGBMæœªå®‰è£…")

# CatBoost
try:
    from catboost import CatBoostClassifier
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    print("è­¦å‘Š: CatBoostæœªå®‰è£…")

warnings.filterwarnings('ignore')

class RNAPredictionPipelineV4:
    """
    ç¯çŠ¶RNAç»„ç»‡è¡¨è¾¾é¢„æµ‹Pipeline v4
    åŸºäºåŸå§‹é«˜åˆ†æ¨¡å‹çš„ä¼˜åŒ–ç‰ˆæœ¬
    """
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        np.random.seed(random_state)
        
        # æ•°æ®å­˜å‚¨
        self.train_data = None
        self.test_data = None
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.test_ids = None
        
        # é¢„å¤„ç†å™¨
        self.encoders = {}
        self.scalers = {}
        self.feature_selector = None
        self.target_encoder = None
        self.feature_names = None
        
        # æ¨¡å‹ç›¸å…³
        self.models = {}
        self.model_results = []
        self.optimized_models = {}
        self.final_model = None
        self.final_model_name = None
        self.final_score = None
        
    def load_data(self, train_path='train.csv', test_path='test.csv'):
        """
        åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        """
        print("åŠ è½½æ•°æ®...")
        print("=" * 50)
        
        # åŠ è½½æ•°æ®
        self.train_data = pd.read_csv(train_path)
        self.test_data = pd.read_csv(test_path)
        
        print(f"è®­ç»ƒé›†å½¢çŠ¶: {self.train_data.shape}")
        print(f"æµ‹è¯•é›†å½¢çŠ¶: {self.test_data.shape}")
        
        # ä¿å­˜æµ‹è¯•é›†ID
        self.test_ids = self.test_data['ID'].values
        
        return self
    
    def analyze_data(self):
        """
        æ•°æ®åˆ†æ
        """
        print("\næ•°æ®åˆ†æ...")
        print("=" * 50)
        
        # åŸºæœ¬ä¿¡æ¯
        print("è®­ç»ƒé›†åŸºæœ¬ä¿¡æ¯:")
        print(f"  æ ·æœ¬æ•°: {len(self.train_data)}")
        print(f"  ç‰¹å¾æ•°: {self.train_data.shape[1] - 2}")  # å‡å»IDå’ŒTissue
        
        # ç›®æ ‡å˜é‡åˆ†å¸ƒ
        tissue_dist = self.train_data['Tissue'].value_counts().sort_index()
        print(f"\nç›®æ ‡å˜é‡åˆ†å¸ƒ:")
        for tissue, count in tissue_dist.items():
            print(f"  {tissue}: {count} ({count/len(self.train_data)*100:.1f}%)")
        
        # ç¼ºå¤±å€¼æ£€æŸ¥
        train_missing = self.train_data.isnull().sum().sum()
        test_missing = self.test_data.isnull().sum().sum()
        print(f"\nç¼ºå¤±å€¼:")
        print(f"  è®­ç»ƒé›†: {train_missing}")
        print(f"  æµ‹è¯•é›†: {test_missing}")
        
        return self
    
    def feature_engineering(self):
        """
        ç‰¹å¾å·¥ç¨‹ - ä¿ç•™åŸå§‹æ¨¡å‹çš„æˆåŠŸç­–ç•¥
        """
        print("\nç‰¹å¾å·¥ç¨‹...")
        print("=" * 50)
        
        # æå–ç‰¹å¾å’Œç›®æ ‡å˜é‡
        feature_cols = [col for col in self.train_data.columns if col not in ['ID', 'Tissue']]
        
        X_train = self.train_data[feature_cols].copy()
        y_train = self.train_data['Tissue'].copy()
        X_test = self.test_data[feature_cols].copy()
        
        print(f"åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
        
        # 1. å¤„ç†åˆ†ç±»ç‰¹å¾ç¼–ç 
        print("å¤„ç†åˆ†ç±»ç‰¹å¾...")
        categorical_features = ['Strand', 'Circtype', 'has_N']
        
        for col in categorical_features:
            if col in feature_cols:
                le = LabelEncoder()
                # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®è¿›è¡Œç¼–ç ï¼Œç¡®ä¿ä¸€è‡´æ€§
                combined_data = pd.concat([X_train[col], X_test[col]], axis=0)
                le.fit(combined_data)
                X_train[col] = le.transform(X_train[col])
                X_test[col] = le.transform(X_test[col])
                self.encoders[col] = le
        
        # 2. å¤„ç†ç¼ºå¤±å€¼ - ä½¿ç”¨ä¸­ä½æ•°å¡«å……ï¼ˆä¿ç•™åŸå§‹ç­–ç•¥ï¼‰
        print("å¤„ç†ç¼ºå¤±å€¼...")
        for col in feature_cols:
            if X_train[col].isnull().sum() > 0:
                median_val = X_train[col].median()
                X_train[col].fillna(median_val, inplace=True)
                X_test[col].fillna(median_val, inplace=True)
        
        # 2. ç›®æ ‡å˜é‡ç¼–ç 
        print("ç¼–ç ç›®æ ‡å˜é‡...")
        self.target_encoder = LabelEncoder()
        y_train_encoded = self.target_encoder.fit_transform(y_train)
        
        # 3. ç‰¹å¾æ ‡å‡†åŒ– - ä½¿ç”¨RobustScalerï¼ˆä¿ç•™åŸå§‹ç­–ç•¥ï¼‰
        print("æ ‡å‡†åŒ–ç‰¹å¾...")
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # è½¬æ¢ä¸ºDataFrameä¿æŒåˆ—å
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols)
        
        # 4. ç‰¹å¾é€‰æ‹© - åŒé‡ç­–ç•¥ï¼ˆä¿ç•™åŸå§‹ç­–ç•¥ä½†ä¼˜åŒ–å‚æ•°ï¼‰
        print("ç‰¹å¾é€‰æ‹©...")
        
        # åŸºäºFç»Ÿè®¡é‡çš„ç‰¹å¾é€‰æ‹©
        k_features_f = min(150, len(feature_cols))  # ç¨å¾®å¢åŠ ç‰¹å¾æ•°
        selector_f = SelectKBest(score_func=f_classif, k=k_features_f)
        X_train_f = selector_f.fit_transform(X_train_scaled, y_train_encoded)
        selected_features_f = X_train_scaled.columns[selector_f.get_support()].tolist()
        
        # åŸºäºäº’ä¿¡æ¯çš„ç‰¹å¾é€‰æ‹©
        k_features_mi = min(150, len(feature_cols))  # ç¨å¾®å¢åŠ ç‰¹å¾æ•°
        selector_mi = SelectKBest(score_func=mutual_info_classif, k=k_features_mi)
        X_train_mi = selector_mi.fit_transform(X_train_scaled, y_train_encoded)
        selected_features_mi = X_train_scaled.columns[selector_mi.get_support()].tolist()
        
        # åˆå¹¶ç‰¹å¾é€‰æ‹©ç»“æœ
        selected_features = list(set(selected_features_f + selected_features_mi))
        
        # åº”ç”¨ç‰¹å¾é€‰æ‹©
        X_train_final = X_train_scaled[selected_features]
        X_test_final = X_test_scaled[selected_features]
        
        print(f"Fç»Ÿè®¡é‡é€‰æ‹©ç‰¹å¾æ•°: {len(selected_features_f)}")
        print(f"äº’ä¿¡æ¯é€‰æ‹©ç‰¹å¾æ•°: {len(selected_features_mi)}")
        print(f"æœ€ç»ˆç‰¹å¾æ•°: {len(selected_features)}")
        
        # ä¿å­˜ç»“æœ
        self.X_train = X_train_final.values
        self.y_train = y_train_encoded
        self.X_test = X_test_final.values
        self.feature_names = selected_features
        self.scalers['robust'] = scaler
        self.feature_selector = {'f_selector': selector_f, 'mi_selector': selector_mi}
        
        return self
    
    def create_models(self):
        """
        åˆ›å»ºåŸºç¡€æ¨¡å‹ - ä¿ç•™åŸå§‹æ¨¡å‹ç»„åˆ
        """
        print("\nåˆ›å»ºåŸºç¡€æ¨¡å‹...")
        print("=" * 50)
        
        # 1. Random Forest
        self.models['RandomForest'] = RandomForestClassifier(
            n_estimators=100,
            random_state=self.random_state,
            n_jobs=-1,
            class_weight='balanced'
        )
        
        # 2. Extra Trees
        self.models['ExtraTrees'] = ExtraTreesClassifier(
            n_estimators=100,
            random_state=self.random_state,
            n_jobs=-1,
            class_weight='balanced'
        )
        
        # 3. XGBoost
        if XGBOOST_AVAILABLE:
            self.models['XGBoost'] = xgb.XGBClassifier(
                random_state=self.random_state,
                n_jobs=-1,
                eval_metric='mlogloss'
            )
        
        # 4. LightGBM
        if LIGHTGBM_AVAILABLE:
            self.models['LightGBM'] = lgb.LGBMClassifier(
                random_state=self.random_state,
                n_jobs=-1,
                verbose=-1
            )
        
        # 5. CatBoost
        if CATBOOST_AVAILABLE:
            self.models['CatBoost'] = CatBoostClassifier(
                random_state=self.random_state,
                verbose=False
            )
        
        # 6. Logistic Regression
        self.models['LogisticRegression'] = LogisticRegression(
            random_state=self.random_state,
            max_iter=1000,
            class_weight='balanced'
        )
        
        print(f"åˆ›å»ºäº† {len(self.models)} ä¸ªåŸºç¡€æ¨¡å‹")
        for name in self.models.keys():
            print(f"  - {name}")
        
        return self
    
    def evaluate_models(self):
        """
        è¯„ä¼°åŸºç¡€æ¨¡å‹
        """
        print("\nè¯„ä¼°åŸºç¡€æ¨¡å‹...")
        print("=" * 50)
        
        cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        results = []
        
        for name, model in self.models.items():
            print(f"\nè¯„ä¼° {name}...")
            
            start_time = time.time()
            cv_scores = cross_val_score(
                model, self.X_train, self.y_train,
                cv=cv_strategy, scoring='f1_macro', n_jobs=-1
            )
            eval_time = time.time() - start_time
            
            result = {
                'model_name': name,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'cv_scores': cv_scores,
                'eval_time': eval_time
            }
            
            results.append(result)
            
            print(f"  CVåˆ†æ•°: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")
            print(f"  è¯„ä¼°æ—¶é—´: {eval_time:.2f}ç§’")
        
        # æŒ‰æ€§èƒ½æ’åº
        results.sort(key=lambda x: x['cv_mean'], reverse=True)
        self.model_results = results
        
        print("\næ¨¡å‹æ€§èƒ½æ’åº:")
        for i, result in enumerate(results, 1):
            print(f"  {i}. {result['model_name']}: {result['cv_mean']:.4f}")
        
        return self
    
    def optimize_best_models(self, top_k=4):
        """
        ä¼˜åŒ–è¡¨ç°æœ€ä½³çš„Kä¸ªæ¨¡å‹ - ç²¾ç»†åŒ–è¶…å‚æ•°ç½‘æ ¼
        """
        print(f"\nä¼˜åŒ–Top {top_k}æ¨¡å‹...")
        print("=" * 50)
        
        top_models = self.model_results[:top_k]
        optimized_models = {}
        
        # ç²¾ç»†åŒ–çš„è¶…å‚æ•°ç½‘æ ¼
        param_grids = {
            'RandomForest': {
                'n_estimators': [100, 200, 300, 500],
                'max_depth': [6, 8, 10, 12, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', 0.8]
            },
            'ExtraTrees': {
                'n_estimators': [100, 200, 300, 500],
                'max_depth': [6, 8, 10, 12, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', 0.8]
            },
            'LogisticRegression': {
                'C': [0.01, 0.1, 1, 10, 100],
                'penalty': ['l1', 'l2', 'elasticnet'],
                'solver': ['liblinear', 'saga'],
                'l1_ratio': [0.1, 0.5, 0.7, 0.9]
            }
        }
        
        # XGBoostå‚æ•°
        if XGBOOST_AVAILABLE:
            param_grids['XGBoost'] = {
                'n_estimators': [100, 200, 300, 500],
                'max_depth': [4, 6, 8, 10],
                'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
                'subsample': [0.7, 0.8, 0.9, 1.0],
                'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
                'reg_alpha': [0, 0.1, 0.5, 1],
                'reg_lambda': [0, 0.1, 0.5, 1]
            }
        
        # LightGBMå‚æ•°
        if LIGHTGBM_AVAILABLE:
            param_grids['LightGBM'] = {
                'n_estimators': [100, 200, 300, 500],
                'max_depth': [4, 6, 8, 10],
                'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
                'num_leaves': [20, 31, 50, 100],
                'subsample': [0.7, 0.8, 0.9, 1.0],
                'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
                'reg_alpha': [0, 0.1, 0.5, 1],
                'reg_lambda': [0, 0.1, 0.5, 1]
            }
        
        # CatBoostå‚æ•°
        if CATBOOST_AVAILABLE:
            param_grids['CatBoost'] = {
                'iterations': [100, 200, 300, 500],
                'depth': [4, 6, 8, 10],
                'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
                'l2_leaf_reg': [1, 3, 5, 7, 9],
                'border_count': [32, 64, 128, 255]
            }
        
        cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)
        
        for model_result in top_models:
            model_name = model_result['model_name']
            
            if model_name not in param_grids:
                print(f"è·³è¿‡ {model_name} (æ— è¶…å‚æ•°ç½‘æ ¼)")
                optimized_models[model_name] = self.models[model_name]
                continue
            
            print(f"\nä¼˜åŒ– {model_name}...")
            
            base_model = self.models[model_name]
            param_grid = param_grids[model_name]
            
            # ä½¿ç”¨RandomizedSearchCVï¼Œå¢åŠ æœç´¢æ¬¡æ•°
            search = RandomizedSearchCV(
                estimator=base_model,
                param_distributions=param_grid,
                n_iter=50,  # å¢åŠ æœç´¢æ¬¡æ•°
                cv=cv_strategy,
                scoring='f1_macro',
                n_jobs=-1,
                random_state=self.random_state,
                verbose=0
            )
            
            start_time = time.time()
            search.fit(self.X_train, self.y_train)
            search_time = time.time() - start_time
            
            optimized_models[model_name] = search.best_estimator_
            
            print(f"  æœ€ä½³CVåˆ†æ•°: {search.best_score_:.4f}")
            print(f"  ä¼˜åŒ–æ—¶é—´: {search_time:.2f}ç§’")
            print(f"  æœ€ä½³å‚æ•°: {search.best_params_}")
        
        self.optimized_models = optimized_models
        return self
    
    def create_ensemble(self):
        """
        åˆ›å»ºé›†æˆæ¨¡å‹ - æ”¹è¿›æƒé‡ç­–ç•¥
        """
        print("\nåˆ›å»ºé›†æˆæ¨¡å‹...")
        print("=" * 50)
        
        # å‡†å¤‡åŸºæ¨¡å‹
        base_models = []
        model_weights = []
        
        # æ ¹æ®CVæ€§èƒ½è®¡ç®—æƒé‡
        for name, model in self.optimized_models.items():
            base_models.append((name, model))
            # æ‰¾åˆ°å¯¹åº”çš„CVåˆ†æ•°
            cv_score = 0.0
            for result in self.model_results:
                if result['model_name'] == name:
                    cv_score = result['cv_mean']
                    break
            model_weights.append(cv_score)
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(model_weights)
        model_weights = [w/total_weight for w in model_weights]
        
        print(f"é›†æˆ {len(base_models)} ä¸ªåŸºæ¨¡å‹")
        for i, (name, _) in enumerate(base_models):
            print(f"  {name}: æƒé‡ {model_weights[i]:.3f}")
        
        # 1. åŠ æƒè½¯æŠ•ç¥¨
        voting_clf = VotingClassifier(
            estimators=base_models,
            voting='soft',
            weights=model_weights,  # ä½¿ç”¨è®¡ç®—çš„æƒé‡
            n_jobs=-1
        )
        
        # 2. Stacking Classifier
        # ä½¿ç”¨æ›´å¼ºçš„å…ƒå­¦ä¹ å™¨
        meta_learner = LogisticRegression(
            random_state=self.random_state,
            max_iter=2000,
            class_weight='balanced',
            C=1.0
        )
        
        stacking_clf = StackingClassifier(
            estimators=base_models,
            final_estimator=meta_learner,
            cv=5,  # å¢åŠ CVæŠ˜æ•°
            n_jobs=-1
        )
        
        # è¯„ä¼°é›†æˆæ¨¡å‹
        cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        print("\nè¯„ä¼°é›†æˆæ¨¡å‹...")
        
        # è¯„ä¼°åŠ æƒVoting
        voting_scores = cross_val_score(
            voting_clf, self.X_train, self.y_train,
            cv=cv_strategy, scoring='f1_macro', n_jobs=-1
        )
        
        print(f"åŠ æƒVoting CV: {voting_scores.mean():.4f} (+/- {voting_scores.std()*2:.4f})")
        
        # è¯„ä¼°Stacking
        stacking_scores = cross_val_score(
            stacking_clf, self.X_train, self.y_train,
            cv=cv_strategy, scoring='f1_macro', n_jobs=-1
        )
        
        print(f"Stacking CV: {stacking_scores.mean():.4f} (+/- {stacking_scores.std()*2:.4f})")
        
        # é€‰æ‹©æœ€ä½³é›†æˆæ–¹æ³•
        if stacking_scores.mean() > voting_scores.mean():
            self.best_ensemble = stacking_clf
            self.best_ensemble_name = 'Stacking'
            self.best_ensemble_score = stacking_scores.mean()
        else:
            self.best_ensemble = voting_clf
            self.best_ensemble_name = 'Weighted Voting'
            self.best_ensemble_score = voting_scores.mean()
        
        print(f"\né€‰æ‹©æœ€ä½³é›†æˆ: {self.best_ensemble_name} (CV: {self.best_ensemble_score:.4f})")
        
        # æ¯”è¾ƒå•æ¨¡å‹å’Œé›†æˆæ¨¡å‹
        best_single_score = max(result['cv_mean'] for result in self.model_results)
        improvement = self.best_ensemble_score - best_single_score
        
        print(f"ç›¸æ¯”æœ€ä½³å•æ¨¡å‹æå‡: {improvement:.4f} ({improvement/best_single_score*100:.2f}%)")
        
        return self
    
    def train_final_model(self):
        """
        è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        """
        print("\nè®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        print("=" * 50)
        
        # é€‰æ‹©æœ€ç»ˆæ¨¡å‹
        best_single_score = max(result['cv_mean'] for result in self.model_results)
        
        if hasattr(self, 'best_ensemble') and self.best_ensemble_score > best_single_score:
            final_model = self.best_ensemble
            final_model_name = f'Ensemble ({self.best_ensemble_name})'
            final_score = self.best_ensemble_score
        else:
            # ä½¿ç”¨æœ€ä½³å•æ¨¡å‹
            best_model_name = self.model_results[0]['model_name']
            final_model = self.optimized_models[best_model_name]
            final_model_name = f'{best_model_name} (Optimized)'
            final_score = self.model_results[0]['cv_mean']
        
        print(f"æœ€ç»ˆæ¨¡å‹: {final_model_name}")
        print(f"é¢„æœŸCVåˆ†æ•°: {final_score:.4f}")
        
        # åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ
        print("åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ...")
        start_time = time.time()
        final_model.fit(self.X_train, self.y_train)
        training_time = time.time() - start_time
        
        print(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
        
        self.final_model = final_model
        self.final_model_name = final_model_name
        self.final_score = final_score
        
        return self
    
    def generate_predictions(self, output_file='submit_v4.csv'):
        """
        ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹ç»“æœ
        """
        print("\nç”Ÿæˆé¢„æµ‹ç»“æœ...")
        print("=" * 50)
        
        # é¢„æµ‹
        print("é¢„æµ‹æµ‹è¯•é›†...")
        y_pred_encoded = self.final_model.predict(self.X_test)
        
        # è§£ç é¢„æµ‹ç»“æœ
        y_pred = self.target_encoder.inverse_transform(y_pred_encoded)
        
        # åˆ›å»ºæäº¤æ–‡ä»¶
        submission = pd.DataFrame({
            'ID': self.test_ids,
            'Tissue': y_pred
        })
        
        # ä¿å­˜
        submission.to_csv(output_file, index=False)
        print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # é¢„æµ‹åˆ†å¸ƒ
        pred_dist = pd.Series(y_pred).value_counts().sort_index()
        print(f"\né¢„æµ‹åˆ†å¸ƒ:")
        for tissue, count in pred_dist.items():
            print(f"  {tissue}: {count} ({count/len(y_pred)*100:.1f}%)")
        
        self.submission = submission
        return self
    
    def save_model(self, model_file='final_model_v4.pkl'):
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œé¢„å¤„ç†å™¨
        """
        print(f"\nä¿å­˜æ¨¡å‹åˆ°: {model_file}")
        
        model_data = {
            'model': self.final_model,
            'model_name': self.final_model_name,
            'cv_score': self.final_score,
            'scalers': self.scalers,
            'feature_selector': self.feature_selector,
            'feature_names': self.feature_names,
            'target_encoder': self.target_encoder,
            'training_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'version': 'v4'
        }
        
        with open(model_file, 'wb') as f:
            pickle.dump(model_data, f)
        
        print("æ¨¡å‹ä¿å­˜å®Œæˆ")
        
        return self
    
    def run_complete_pipeline(self, train_path='train.csv', test_path='test.csv', 
                            output_file='submit_v4.csv', model_file='final_model_v4.pkl'):
        """
        è¿è¡Œå®Œæ•´çš„é¢„æµ‹pipeline
        """
        print("=" * 80)
        print("           ç¯çŠ¶RNAç»„ç»‡è¡¨è¾¾é¢„æµ‹ - Pipeline v4")
        print("=" * 80)
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œå®Œæ•´æµç¨‹
            (self.load_data(train_path, test_path)
             .analyze_data()
             .feature_engineering()
             .create_models()
             .evaluate_models()
             .optimize_best_models()
             .create_ensemble()
             .train_final_model()
             .generate_predictions(output_file)
             .save_model(model_file))
            
            total_time = time.time() - start_time
            
            print("\n" + "=" * 80)
            print("                        Pipeline v4 å®Œæˆ")
            print("=" * 80)
            print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
            print(f"æœ€ç»ˆæ¨¡å‹: {self.final_model_name}")
            print(f"é¢„æœŸæ€§èƒ½: {self.final_score:.4f} (Macro-F1)")
            print(f"é¢„æµ‹æ–‡ä»¶: {output_file}")
            print(f"æ¨¡å‹æ–‡ä»¶: {model_file}")
            print("=" * 80)
            
            return True
            
        except Exception as e:
            print(f"\nPipelineæ‰§è¡Œå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """
    ä¸»å‡½æ•°
    """
    # åˆ›å»ºpipelineå®ä¾‹
    pipeline = RNAPredictionPipelineV4(random_state=42)
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    success = pipeline.run_complete_pipeline(
        train_path='train.csv',
        test_path='test.csv',
        output_file='submit_v4.csv',
        model_file='final_model_v4.pkl'
    )
    
    if success:
        print("\nğŸ‰ é¢„æµ‹ä»»åŠ¡å®Œæˆï¼")
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   - submit_v4.csv: æäº¤æ–‡ä»¶")
        print("   - final_model_v4.pkl: è®­ç»ƒå¥½çš„æ¨¡å‹")
        print("\nğŸ’¡ v4ç‰ˆæœ¬æ”¹è¿›:")
        print("   - ä¿ç•™åŸå§‹æ¨¡å‹æˆåŠŸæ¶æ„")
        print("   - ä¼˜åŒ–ç‰¹å¾é€‰æ‹©ç­–ç•¥")
        print("   - ç²¾ç»†åŒ–è¶…å‚æ•°ç½‘æ ¼")
        print("   - æ”¹è¿›é›†æˆæƒé‡ç­–ç•¥")
    else:
        print("\nâŒ é¢„æµ‹ä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()