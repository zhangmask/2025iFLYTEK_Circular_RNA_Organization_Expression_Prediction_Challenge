{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环状RNA组织表达预测 - 特征工程\n",
    "\n",
    "本notebook用于对环状RNA数据进行特征工程和数据预处理，包括：\n",
    "1. 数据加载和预处理\n",
    "2. 分类特征编码\n",
    "3. 数值特征标准化\n",
    "4. 特征选择\n",
    "5. 新特征构建\n",
    "6. 数据分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# 设置显示选项\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"训练集形状: {train_df.shape}\")\n",
    "print(f\"测试集形状: {test_df.shape}\")\n",
    "\n",
    "# 加载探索结果\n",
    "try:\n",
    "    with open('exploration_results.json', 'r', encoding='utf-8') as f:\n",
    "        exploration_results = json.load(f)\n",
    "    print(\"\\n已加载数据探索结果\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n未找到exploration_results.json，将重新分析特征类型\")\n",
    "    exploration_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义特征类型\n",
    "target_col = 'Tissue'\n",
    "id_col = 'ID'\n",
    "\n",
    "if exploration_results:\n",
    "    numeric_features = exploration_results['numeric_features']\n",
    "    categorical_features = exploration_results['categorical_features']\n",
    "else:\n",
    "    # 重新分析特征类型\n",
    "    numeric_features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if id_col in numeric_features:\n",
    "        numeric_features.remove(id_col)\n",
    "    \n",
    "    categorical_features = train_df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "    if target_col in categorical_features:\n",
    "        categorical_features.remove(target_col)\n",
    "\n",
    "print(f\"数值特征: {numeric_features}\")\n",
    "print(f\"分类特征: {categorical_features}\")\n",
    "print(f\"目标变量: {target_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查和处理缺失值\n",
    "print(\"训练集缺失值:\")\n",
    "train_missing = train_df.isnull().sum()\n",
    "print(train_missing[train_missing > 0])\n",
    "\n",
    "print(\"测试集缺失值:\")\n",
    "test_missing = test_df.isnull().sum()\n",
    "print(test_missing[test_missing > 0])\n",
    "\n",
    "# 如果有缺失值，进行处理\n",
    "if train_missing.sum() > 0 or test_missing.sum() > 0:\n",
    "    print(\"\\n处理缺失值...\")\n",
    "    \n",
    "    # 对数值特征用中位数填充\n",
    "    for feature in numeric_features:\n",
    "        if train_df[feature].isnull().sum() > 0:\n",
    "            median_val = train_df[feature].median()\n",
    "            train_df[feature].fillna(median_val, inplace=True)\n",
    "            test_df[feature].fillna(median_val, inplace=True)\n",
    "            print(f\"  {feature}: 用中位数 {median_val} 填充\")\n",
    "    \n",
    "    # 对分类特征用众数填充\n",
    "    for feature in categorical_features:\n",
    "        if train_df[feature].isnull().sum() > 0:\n",
    "            mode_val = train_df[feature].mode()[0]\n",
    "            train_df[feature].fillna(mode_val, inplace=True)\n",
    "            test_df[feature].fillna(mode_val, inplace=True)\n",
    "            print(f\"  {feature}: 用众数 {mode_val} 填充\")\n",
    "else:\n",
    "    print(\"无缺失值，跳过填充步骤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 分类特征编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据副本用于特征工程\n",
    "train_processed = train_df.copy()\n",
    "test_processed = test_df.copy()\n",
    "\n",
    "# 存储编码器\n",
    "encoders = {}\n",
    "\n",
    "# 对分类特征进行标签编码\n",
    "if len(categorical_features) > 0:\n",
    "    print(\"对分类特征进行标签编码:\")\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        # 合并训练集和测试集的唯一值来拟合编码器\n",
    "        all_values = pd.concat([train_processed[feature], test_processed[feature]]).unique()\n",
    "        le.fit(all_values)\n",
    "        \n",
    "        # 应用编码\n",
    "        train_processed[feature] = le.transform(train_processed[feature])\n",
    "        test_processed[feature] = le.transform(test_processed[feature])\n",
    "        \n",
    "        # 保存编码器\n",
    "        encoders[feature] = le\n",
    "        \n",
    "        print(f\"  {feature}: {len(le.classes_)} 个类别\")\n",
    "        print(f\"    类别: {le.classes_}\")\n",
    "\n",
    "# 对目标变量进行编码\n",
    "target_encoder = LabelEncoder()\n",
    "y_train = target_encoder.fit_transform(train_processed[target_col])\n",
    "encoders['target'] = target_encoder\n",
    "\n",
    "print(f\"\\n目标变量编码:\")\n",
    "print(f\"类别数: {len(target_encoder.classes_)}\")\n",
    "print(f\"类别: {target_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 特征构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于领域知识构建新特征\n",
    "def create_new_features(df):\n",
    "    \"\"\"创建新特征\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # 1. GC含量相关特征\n",
    "    if 'GC_content' in df_new.columns:\n",
    "        df_new['AT_content'] = 1 - df_new['GC_content']  # AT含量\n",
    "        df_new['GC_AT_ratio'] = df_new['GC_content'] / (df_new['AT_content'] + 1e-8)  # GC/AT比例\n",
    "    \n",
    "    # 2. 碱基比例相关特征\n",
    "    if all(col in df_new.columns for col in ['A_ratio', 'T_ratio', 'G_ratio', 'C_ratio']):\n",
    "        df_new['purine_ratio'] = df_new['A_ratio'] + df_new['G_ratio']  # 嘌呤比例\n",
    "        df_new['pyrimidine_ratio'] = df_new['T_ratio'] + df_new['C_ratio']  # 嘧啶比例\n",
    "        df_new['purine_pyrimidine_ratio'] = df_new['purine_ratio'] / (df_new['pyrimidine_ratio'] + 1e-8)\n",
    "        \n",
    "        # 强弱氢键比例\n",
    "        df_new['weak_bond_ratio'] = df_new['A_ratio'] + df_new['T_ratio']  # AT弱氢键\n",
    "        df_new['strong_bond_ratio'] = df_new['G_ratio'] + df_new['C_ratio']  # GC强氢键\n",
    "        df_new['bond_strength_ratio'] = df_new['strong_bond_ratio'] / (df_new['weak_bond_ratio'] + 1e-8)\n",
    "    \n",
    "    # 3. 长度相关特征\n",
    "    if 'length' in df_new.columns:\n",
    "        df_new['log_length'] = np.log1p(df_new['length'])  # 对数长度\n",
    "        \n",
    "        # 长度分组\n",
    "        df_new['length_group'] = pd.cut(df_new['length'], \n",
    "                                       bins=[0, 500, 1000, 2000, float('inf')], \n",
    "                                       labels=[0, 1, 2, 3])\n",
    "        df_new['length_group'] = df_new['length_group'].astype(int)\n",
    "    \n",
    "    # 4. miRNA结合相关特征\n",
    "    if all(col in df_new.columns for col in ['miRNA Binding count', 'length']):\n",
    "        df_new['miRNA_density'] = df_new['miRNA Binding count'] / (df_new['length'] + 1)  # miRNA结合密度\n",
    "    \n",
    "    if all(col in df_new.columns for col in ['miRNA Binding count', 'Average free energy']):\n",
    "        # 平均每个miRNA的自由能\n",
    "        df_new['energy_per_miRNA'] = df_new['Average free energy'] / (df_new['miRNA Binding count'] + 1)\n",
    "    \n",
    "    # 5. 自由能相关特征\n",
    "    if 'Average free energy' in df_new.columns:\n",
    "        df_new['abs_free_energy'] = np.abs(df_new['Average free energy'])  # 绝对自由能\n",
    "        df_new['log_abs_free_energy'] = np.log1p(df_new['abs_free_energy'])\n",
    "        \n",
    "        if 'length' in df_new.columns:\n",
    "            df_new['energy_density'] = df_new['abs_free_energy'] / (df_new['length'] + 1)  # 能量密度\n",
    "    \n",
    "    # 6. 组合特征\n",
    "    if all(col in df_new.columns for col in ['GC_content', 'length']):\n",
    "        df_new['GC_length_interaction'] = df_new['GC_content'] * np.log1p(df_new['length'])\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# 应用特征构建\n",
    "print(\"构建新特征...\")\n",
    "train_processed = create_new_features(train_processed)\n",
    "test_processed = create_new_features(test_processed)\n",
    "\n",
    "# 更新特征列表\n",
    "all_features = [col for col in train_processed.columns if col not in [id_col, target_col]]\n",
    "new_features = [col for col in all_features if col not in numeric_features + categorical_features]\n",
    "\n",
    "print(f\"原始特征数: {len(numeric_features + categorical_features)}\")\n",
    "print(f\"新增特征数: {len(new_features)}\")\n",
    "print(f\"总特征数: {len(all_features)}\")\n",
    "print(f\"新增特征: {new_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 特征标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备特征矩阵\n",
    "X_train = train_processed[all_features].copy()\n",
    "X_test = test_processed[all_features].copy()\n",
    "\n",
    "print(f\"特征矩阵形状:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "\n",
    "# 使用RobustScaler进行标准化（对异常值更鲁棒）\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 转换回DataFrame\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=all_features, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=all_features, index=X_test.index)\n",
    "\n",
    "print(\"\\n特征标准化完成\")\n",
    "print(f\"标准化后特征统计:\")\n",
    "print(X_train_scaled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用多种方法进行特征选择\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. 基于F统计量的特征选择\n",
    "selector_f = SelectKBest(score_func=f_classif, k='all')\n",
    "selector_f.fit(X_train_scaled, y_train)\n",
    "f_scores = selector_f.scores_\n",
    "\n",
    "# 2. 基于互信息的特征选择\n",
    "selector_mi = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "selector_mi.fit(X_train_scaled, y_train)\n",
    "mi_scores = selector_mi.scores_\n",
    "\n",
    "# 3. 基于随机森林的特征重要性\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_selector.fit(X_train_scaled, y_train)\n",
    "rf_scores = rf_selector.feature_importances_\n",
    "\n",
    "# 创建特征重要性DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'f_score': f_scores,\n",
    "    'mi_score': mi_scores,\n",
    "    'rf_importance': rf_scores\n",
    "})\n",
    "\n",
    "# 标准化分数到0-1范围\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_scores = MinMaxScaler()\n",
    "feature_importance_df[['f_score_norm', 'mi_score_norm', 'rf_importance_norm']] = scaler_scores.fit_transform(\n",
    "    feature_importance_df[['f_score', 'mi_score', 'rf_importance']]\n",
    ")\n",
    "\n",
    "# 计算综合分数\n",
    "feature_importance_df['combined_score'] = (\n",
    "    feature_importance_df['f_score_norm'] * 0.3 + \n",
    "    feature_importance_df['mi_score_norm'] * 0.3 + \n",
    "    feature_importance_df['rf_importance_norm'] * 0.4\n",
    ")\n",
    "\n",
    "# 按综合分数排序\n",
    "feature_importance_df = feature_importance_df.sort_values('combined_score', ascending=False)\n",
    "\n",
    "print(\"特征重要性排序 (Top 20):\")\n",
    "display(feature_importance_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化特征重要性\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 选择前20个最重要的特征进行可视化\n",
    "top_features = feature_importance_df.head(20)\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.barh(range(len(top_features)), top_features['f_score_norm'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('F-Score (Normalized)')\n",
    "plt.title('F-统计量特征重要性')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.barh(range(len(top_features)), top_features['mi_score_norm'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Mutual Information (Normalized)')\n",
    "plt.title('互信息特征重要性')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.barh(range(len(top_features)), top_features['rf_importance_norm'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Random Forest Importance (Normalized)')\n",
    "plt.title('随机森林特征重要性')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.barh(range(len(top_features)), top_features['combined_score'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Combined Score')\n",
    "plt.title('综合特征重要性')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 数据分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割训练集和验证集\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_scaled, y_train, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"数据分割结果:\")\n",
    "print(f\"训练集: {X_train_split.shape}\")\n",
    "print(f\"验证集: {X_val_split.shape}\")\n",
    "print(f\"测试集: {X_test_scaled.shape}\")\n",
    "\n",
    "# 检查类别分布\n",
    "print(f\"\\n训练集类别分布:\")\n",
    "train_dist = pd.Series(y_train_split).value_counts().sort_index()\n",
    "print(train_dist)\n",
    "\n",
    "print(f\"\\n验证集类别分布:\")\n",
    "val_dist = pd.Series(y_val_split).value_counts().sort_index()\n",
    "print(val_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 保存处理后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存处理后的数据\n",
    "processed_data = {\n",
    "    'X_train': X_train_scaled,\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_train': y_train,\n",
    "    'X_train_split': X_train_split,\n",
    "    'X_val_split': X_val_split,\n",
    "    'y_train_split': y_train_split,\n",
    "    'y_val_split': y_val_split,\n",
    "    'feature_names': all_features,\n",
    "    'feature_importance': feature_importance_df,\n",
    "    'test_ids': test_df[id_col].values\n",
    "}\n",
    "\n",
    "# 保存为pickle文件\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "# 保存预处理器\n",
    "preprocessors = {\n",
    "    'encoders': encoders,\n",
    "    'scaler': scaler,\n",
    "    'feature_importance': feature_importance_df\n",
    "}\n",
    "\n",
    "with open('preprocessors.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessors, f)\n",
    "\n",
    "print(\"处理后的数据已保存到:\")\n",
    "print(\"- processed_data.pkl\")\n",
    "print(\"- preprocessors.pkl\")\n",
    "\n",
    "# 保存特征重要性到CSV\n",
    "feature_importance_df.to_csv('feature_importance.csv', index=False)\n",
    "print(\"- feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 特征工程总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程总结\n",
    "print(\"=" * 60)\n",
    "print(\"           特征工程总结报告\")\n",
    "print(\"=" * 60)\n",
    "\n",
    "print(f\"1. 数据预处理:\")\n",
    "print(f\"   - 缺失值处理: 完成\")\n",
    "print(f\"   - 分类特征编码: {len(categorical_features)} 个特征\")\n",
    "print(f\"   - 数值特征标准化: RobustScaler\")\n",
    "\n",
    "print(f\"2. 特征构建:\")\n",
    "print(f\"   - 原始特征数: {len(numeric_features + categorical_features)}\")\n",
    "print(f\"   - 新增特征数: {len(new_features)}\")\n",
    "print(f\"   - 总特征数: {len(all_features)}\")\n",
    "\n",
    "print(f\"3. 特征选择:\")\n",
    "print(f\"   - F统计量评分: 完成\")\n",
    "print(f\"   - 互信息评分: 完成\")\n",
    "print(f\"   - 随机森林重要性: 完成\")\n",
    "print(f\"   - 综合评分: 完成\")\n",
    "\n",
    "print(f\"4. 数据分割:\")\n",
    "print(f\"   - 训练集: {X_train_split.shape[0]} 样本\")\n",
    "print(f\"   - 验证集: {X_val_split.shape[0]} 样本\")\n",
    "print(f\"   - 测试集: {X_test_scaled.shape[0]} 样本\")\n",
    "\n",
    "print(f\"5. Top 10 重要特征:\")\n",
    "for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows(), 1):\n",
    "    print(f\"   {i:2d}. {row['feature']:25s} (分数: {row['combined_score']:.4f})\")\n",
    "\n",
    "print(\"=" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}